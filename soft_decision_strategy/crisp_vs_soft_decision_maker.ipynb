{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape =  (5333, 2)\n",
      "Validation data shape =  (1150, 2)\n",
      "Test data shape =  (1146, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "((7619, 14), (5326, 14), (1148, 14), (1145, 14))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Reading the bert output\n",
    "\"\"\"\n",
    "bert_all = pd.read_csv(\"predictions-with-labels-all.csv\", header=None)\n",
    "bert_test = pd.read_csv(\"predictions-with-labels-test.csv\", header=None)\n",
    "bert_train = pd.read_csv(\"predictions-with-labels-train.csv\", header=None)\n",
    "bert_val = pd.read_csv(\"predictions-with-labels-val.csv\", header=None)\n",
    "\n",
    "\"\"\"\n",
    "Reading the train, val , test data\n",
    "\"\"\"\n",
    "train = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-train.csv\", on_bad_lines='skip', header=None)\n",
    "print(\"Training data shape = \", train.shape)\n",
    "val = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-val.csv\", header=None)\n",
    "print(\"Validation data shape = \", val.shape)\n",
    "test = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-test.csv\", header=None)\n",
    "print(\"Test data shape = \", test.shape)\n",
    "\n",
    "\"\"\"\n",
    "applying assertion\n",
    "\"\"\"\n",
    "assert bert_all.shape[0] == (bert_train.shape[0] + bert_val.shape[0] + bert_test.shape[0])\n",
    "bert_all.shape, bert_train.shape, bert_val.shape, bert_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ninp_data = test\\nbert_out = bert_test\\n\\nassert inp_data.shape[0] == bert_out.shape[0]\\n\\nanalysis_df = inp_data.merge(bert_out, left_index=True, right_index=True)\\nanalysis_df = analysis_df[analysis_df[\"anger\"]]\\n\\n'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Applying analysis on test\n",
    "\"\"\"\n",
    "\n",
    "# inp_data = pd.concat([test, val])\n",
    "# bert_out = pd.concat([bert_test, bert_val])\n",
    "\n",
    "\n",
    "inp_data = test\n",
    "bert_out = bert_test\n",
    "\n",
    "assert inp_data.shape[0] == bert_out.shape[0]\n",
    "\n",
    "analysis_df = inp_data.merge(bert_out, left_index=True, right_index=True)\n",
    "analysis_df = analysis_df[analysis_df[\"anger\"]]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0    -4.143373 -4.099311 -4.180825 -3.873905  3.270709 -3.883025 -3.976190   \n1    -4.454488  2.634034 -3.970481 -3.600777 -4.260800 -3.508614 -4.351669   \n2    -3.871844 -4.237987 -4.067472 -3.916490  3.443163 -4.022819 -3.743179   \n3    -4.045258  3.200204 -3.871938 -3.878467 -4.119332 -3.877209 -3.389095   \n4    -4.648178 -4.786387 -1.271109 -2.308218 -4.468682  0.195158 -2.507853   \n...        ...       ...       ...       ...       ...       ...       ...   \n7614 -3.077817 -5.027366 -3.306948  1.170537 -5.571788 -1.354663 -2.787492   \n7615 -4.497149  2.975088 -3.899056 -3.790592 -4.291361 -3.033495 -3.544418   \n7616  2.665590 -4.259548 -3.877426 -3.458135 -3.568334 -3.607715 -3.872411   \n7617 -4.251548 -4.527634 -3.848884 -4.058742  3.007751 -3.362574 -4.002860   \n7618 -3.587851 -3.542270 -3.460089 -3.469225 -3.664184 -3.535312  3.006108   \n\n       7    8    9    10   11   12   13  \n0     0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n1     0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n2     0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n3     0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n4     0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n...   ...  ...  ...  ...  ...  ...  ...  \n7614  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n7615  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n7616  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n7617  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n7618  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n\n[7619 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.143373</td>\n      <td>-4.099311</td>\n      <td>-4.180825</td>\n      <td>-3.873905</td>\n      <td>3.270709</td>\n      <td>-3.883025</td>\n      <td>-3.976190</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.454488</td>\n      <td>2.634034</td>\n      <td>-3.970481</td>\n      <td>-3.600777</td>\n      <td>-4.260800</td>\n      <td>-3.508614</td>\n      <td>-4.351669</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-3.871844</td>\n      <td>-4.237987</td>\n      <td>-4.067472</td>\n      <td>-3.916490</td>\n      <td>3.443163</td>\n      <td>-4.022819</td>\n      <td>-3.743179</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-4.045258</td>\n      <td>3.200204</td>\n      <td>-3.871938</td>\n      <td>-3.878467</td>\n      <td>-4.119332</td>\n      <td>-3.877209</td>\n      <td>-3.389095</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4.648178</td>\n      <td>-4.786387</td>\n      <td>-1.271109</td>\n      <td>-2.308218</td>\n      <td>-4.468682</td>\n      <td>0.195158</td>\n      <td>-2.507853</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7614</th>\n      <td>-3.077817</td>\n      <td>-5.027366</td>\n      <td>-3.306948</td>\n      <td>1.170537</td>\n      <td>-5.571788</td>\n      <td>-1.354663</td>\n      <td>-2.787492</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7615</th>\n      <td>-4.497149</td>\n      <td>2.975088</td>\n      <td>-3.899056</td>\n      <td>-3.790592</td>\n      <td>-4.291361</td>\n      <td>-3.033495</td>\n      <td>-3.544418</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7616</th>\n      <td>2.665590</td>\n      <td>-4.259548</td>\n      <td>-3.877426</td>\n      <td>-3.458135</td>\n      <td>-3.568334</td>\n      <td>-3.607715</td>\n      <td>-3.872411</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7617</th>\n      <td>-4.251548</td>\n      <td>-4.527634</td>\n      <td>-3.848884</td>\n      <td>-4.058742</td>\n      <td>3.007751</td>\n      <td>-3.362574</td>\n      <td>-4.002860</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7618</th>\n      <td>-3.587851</td>\n      <td>-3.542270</td>\n      <td>-3.460089</td>\n      <td>-3.469225</td>\n      <td>-3.664184</td>\n      <td>-3.535312</td>\n      <td>3.006108</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7619 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mlp_classifier(X, y):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \"\"\"Perform PCA\"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    oversample = SMOTE()\n",
    "\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    print(y.value_counts())\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_data = pca.fit_transform(X)\n",
    "    reduced_data_df = pd.DataFrame(reduced_data, columns=[\"dim1\", \"dim2\", \"dim3\"])\n",
    "    #reduced_data_df[\"1-2\"] = X[1] - X[2]\n",
    "    #reduced_data_df[\"2-3\"] = X[2] -X[3]\n",
    "    #reduced_data_df[\"3-4\"] = X[3] -X[4]\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    data_col = reduced_data_df.columns\n",
    "    scalar = StandardScaler()\n",
    "    #scalar = MinMaxScaler()\n",
    "    scalar.fit(reduced_data_df)\n",
    "    # transform data\n",
    "    standard_data = scalar.transform(reduced_data_df)\n",
    "    standard_df = pd.DataFrame(standard_data, columns=data_col)\n",
    "    print(standard_df.shape)\n",
    "    print(len(y))\n",
    "    \"\"\"\n",
    "    Data Splitting\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(standard_df, y, test_size = 0.2,\n",
    "                                                        random_state = 42, stratify = y)\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    \"\"\"Model training\"\"\"\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from numpy import loadtxt\n",
    "    from xgboost import XGBClassifier\n",
    "    # fit model no training data\n",
    "    #clf = XGBClassifier()\n",
    "    #\\\\MLP with random_state=1, max_iter=300, giving higest\n",
    "    clf = MLPClassifier(max_iter=100, random_state=42, activation=\"tanh\", solver=\"adam\", hidden_layer_sizes=(100, ),\n",
    "    learning_rate=\"adaptive\")\n",
    "    #clf = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "    #clf = SVC(kernel ='rbf', probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #clf.fit(X_train, y_train)\n",
    "    #clf= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )\n",
    "    #clf.fit(X_train, y_train)\n",
    "    #model = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "    # parameter_space = {\n",
    "    #     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    #     'activation': ['tanh', 'relu'],\n",
    "    #     'solver': ['sgd', 'adam'],\n",
    "    #     'alpha': [0.0001, 0.05],\n",
    "    #     'learning_rate': ['constant','adaptive'],\n",
    "    # }\n",
    "    # from sklearn.model_selection import GridSearchCV\n",
    "    #\n",
    "    # clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "    # clf.fit(X_train, y_train)\n",
    "    #\n",
    "    # # Best paramete set\n",
    "    # print('Best parameters found:\\n', clf.best_params_)\n",
    "    #\n",
    "    # # All results\n",
    "    # means = clf.cv_results_['mean_test_score']\n",
    "    # stds = clf.cv_results_['std_test_score']\n",
    "    # for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    #     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "    \"\"\"Predictions\"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_train = clf.predict(standard_df)\n",
    "    y_pred_train_proba = clf.predict_proba(standard_df)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y, y_pred_train)\n",
    "    print(\"Train Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(classification_report(y, y_pred_train))\n",
    "\n",
    "    print(confusion_matrix(y, y_pred_train))\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    top_df_test = X_test\n",
    "    top_df_test[\"yhat\"] = y_pred\n",
    "    top_df_test[[\"y_pred_prob_1\", \"y_pred_prob_2\"]] = y_pred_prob\n",
    "    top_df_test[\"ytrue\"] = y_test\n",
    "    top_match = []\n",
    "    for ind, row in top_df_test.iterrows():\n",
    "        if row[\"ytrue\"] == row[\"yhat\"]:\n",
    "            top_match.append(1)\n",
    "        else:\n",
    "            top_match.append(0)\n",
    "    top_df_test[\"match\"] = top_match\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return top_df_test, pca, scalar, clf\n",
    "\n",
    "\"\"\"\n",
    "define y\n",
    "\"\"\"\n",
    "# transform the dataset\n",
    "import pickle\n",
    "a, pca, scalar, clf = mlp_classifier(X, match[0])\n",
    "pickle.dump(pca, open(\"pca.p\", 'wb'))\n",
    "pickle.dump(scalar, open(\"scalar.p\", 'wb'))\n",
    "pickle.dump(clf, open(\"crisp_vs_soft_dm.p\", 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Strategy to approach the research question:\n",
    "    1. Filter the dataset (all) to find misclassified examples.\n",
    "    2. Find out the those specific sentences and see manually, if they are the potential examples of class overlapping.\n",
    "    3. Train the ORD detector, and see of the correct label for missclassfied classes are in the top 3 recommendation or not?\n",
    "    4. Evaluate the bert model, using our custom evaluator\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "As discussed with Prof, go with approach one and put approach 2 in future work\n",
    "ORD:\n",
    "\n",
    "1. BERT\n",
    "2. Low_vs_high confidence detector\n",
    "\n",
    "Soft decision maker:\n",
    "\n",
    "1. Approach 1: Rule Based\n",
    "\n",
    "    If overlapping region detected:\n",
    "\n",
    "    Recommend top 2 emotions\n",
    "\n",
    "    If non overlapping region detected:\n",
    "\n",
    "    Recommend top emotion\n",
    "\n",
    "2. Approach 2: Machine learning based\n",
    "    1. Train Classifiers:\n",
    "        1. Classifier 1 checks if top 2 can be recommended.\n",
    "        2. Classifier 2 checks, if top 3 can be recommended.\n",
    "        3. Worst case senario: model not confident with the emotion.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}