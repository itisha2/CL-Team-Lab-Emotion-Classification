{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Reading the bert output\n",
    "\"\"\"\n",
    "\n",
    "mapping = {1: \"fear\", 2: \"anger\", 3: \"guilt\", 4: \"joy\", 5: \"shame\", 6: \"disgust\", 0: \"sadness\"}\n",
    "\n",
    "drop_col = [9, 10, 11, 12, 13, 14, 15]\n",
    "bert_train = pd.read_csv(\"train-16.csv\", header=None, on_bad_lines='skip', delimiter=\"|\").drop(columns=drop_col)\n",
    "bert_val = pd.read_csv(\"val-16.csv\", header=None, on_bad_lines='skip', delimiter=\"|\").drop(columns=drop_col)\n",
    "bert_test = pd.read_csv(\"test-16.csv\", header=None, on_bad_lines='skip', delimiter=\"|\").drop(columns=drop_col)\n",
    "\n",
    "bert_all = pd.concat([bert_train, bert_val, bert_test])\n",
    "\"\"\"\n",
    "Reading the train, val , test data\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "train = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-train.csv\", on_bad_lines='skip', header=None)\n",
    "print(\"Training data shape = \", train.shape)\n",
    "val = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-val.csv\", header=None)\n",
    "print(\"Validation data shape = \", val.shape)\n",
    "test = pd.read_csv(\"/Users/ItishaYadav1/CL-Team-Lab-Emotion-Classification/data/isear/isear-test.csv\", header=None)\n",
    "print(\"Test data shape = \", test.shape)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "def get_prediction(df):\n",
    "    pred = []\n",
    "    match = []\n",
    "    hash = []\n",
    "    conf = []\n",
    "    for ind, row in df.iterrows():\n",
    "        m = 0\n",
    "        hash_object = hashlib.sha256(row[0].encode('utf-8'))\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "        hash.append(hex_dig)\n",
    "        proba = list(row[[2, 3, 4, 5, 6, 7, 8]])\n",
    "        largest = np.argmax(proba)\n",
    "        if float(largest) == row[1]:\n",
    "            m = 1\n",
    "        match.append(m)\n",
    "        pred.append(float(largest))\n",
    "        proba.sort(reverse=True)\n",
    "        conf.append([hex_dig, m] + proba)\n",
    "\n",
    "    df[\"predictions\"] = pred\n",
    "    df[\"match\"] = match\n",
    "    df[\"_id\"] = hash\n",
    "    df = df.rename(columns={0: \"sentence\", 1: \"ytrue\"})\n",
    "\n",
    "    return df, pd.DataFrame(conf)\n",
    "\n",
    "bert_all, conf_df = get_prediction(bert_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               sentence  ytrue         2  \\\n0     When I understood that I was admitted to the U...    4.0 -4.143373   \n1     I broke a window of a neighbouring house and I...    1.0 -4.454488   \n2                            Got a big fish in fishing.    4.0 -3.871844   \n3     Whenever I am alone in a dark room, walk alone...    1.0 -4.045258   \n4     I bought a possible answer to a homework probl...    5.0 -4.648178   \n...                                                 ...    ...       ...   \n1140  Not being a good host for a friend from a far ...    3.0 -3.077817   \n1141  I had a very horrible dream one night, I dream...    1.0 -4.497149   \n1142  My sweetheart left me, or rather we decided to...    0.0  2.665590   \n1143                   My boyfriend made me a proposal.    4.0 -4.251548   \n1144  I am disgusted with liers, hypocrites, slander...    6.0 -3.587851   \n\n             3         4         5         6         7         8  predictions  \\\n0    -4.099311 -4.180825 -3.873905  3.270709 -3.883025 -3.976190          4.0   \n1     2.634034 -3.970481 -3.600777 -4.260800 -3.508614 -4.351669          1.0   \n2    -4.237987 -4.067472 -3.916490  3.443163 -4.022819 -3.743179          4.0   \n3     3.200204 -3.871938 -3.878467 -4.119332 -3.877209 -3.389095          1.0   \n4    -4.786387 -1.271109 -2.308218 -4.468682  0.195158 -2.507853          5.0   \n...        ...       ...       ...       ...       ...       ...          ...   \n1140 -5.027366 -3.306948  1.170537 -5.571788 -1.354663 -2.787492          3.0   \n1141  2.975088 -3.899056 -3.790592 -4.291361 -3.033495 -3.544418          1.0   \n1142 -4.259548 -3.877426 -3.458135 -3.568334 -3.607715 -3.872411          0.0   \n1143 -4.527634 -3.848884 -4.058742  3.007751 -3.362574 -4.002860          4.0   \n1144 -3.542270 -3.460089 -3.469225 -3.664184 -3.535312  3.006108          6.0   \n\n      match                                                _id  \n0         1  57496f41a1283e64bcd582095e0121d32d9f297a9526e5...  \n1         1  9f5abc554b66183e2b9de975feb3b9f02975857c4b6663...  \n2         1  bdce673da042f3c3297dde73b1c5bfade724f8e508d3fc...  \n3         1  490a81728c3228478e8f9196a09d485cd8e06b8c0512c4...  \n4         1  7db158133f1a4d68c59dc693051bfe63c47c80d906dc01...  \n...     ...                                                ...  \n1140      1  e9534a6e936543dcc8437a79899a804ffce79dac3054ee...  \n1141      1  d1a9fe0005e2d8b96fe3a97c87621da377fd26d585efd4...  \n1142      1  9715d835adb73aac1dbf805e9e8474997a614da4222656...  \n1143      1  9b088be713afaffe47f1b751f79cf0e584f93d8613819e...  \n1144      1  a39a81be5c0586a3eb01525370fefbaee556665c8a1099...  \n\n[7619 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>ytrue</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>predictions</th>\n      <th>match</th>\n      <th>_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When I understood that I was admitted to the U...</td>\n      <td>4.0</td>\n      <td>-4.143373</td>\n      <td>-4.099311</td>\n      <td>-4.180825</td>\n      <td>-3.873905</td>\n      <td>3.270709</td>\n      <td>-3.883025</td>\n      <td>-3.976190</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>57496f41a1283e64bcd582095e0121d32d9f297a9526e5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I broke a window of a neighbouring house and I...</td>\n      <td>1.0</td>\n      <td>-4.454488</td>\n      <td>2.634034</td>\n      <td>-3.970481</td>\n      <td>-3.600777</td>\n      <td>-4.260800</td>\n      <td>-3.508614</td>\n      <td>-4.351669</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>9f5abc554b66183e2b9de975feb3b9f02975857c4b6663...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Got a big fish in fishing.</td>\n      <td>4.0</td>\n      <td>-3.871844</td>\n      <td>-4.237987</td>\n      <td>-4.067472</td>\n      <td>-3.916490</td>\n      <td>3.443163</td>\n      <td>-4.022819</td>\n      <td>-3.743179</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>bdce673da042f3c3297dde73b1c5bfade724f8e508d3fc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Whenever I am alone in a dark room, walk alone...</td>\n      <td>1.0</td>\n      <td>-4.045258</td>\n      <td>3.200204</td>\n      <td>-3.871938</td>\n      <td>-3.878467</td>\n      <td>-4.119332</td>\n      <td>-3.877209</td>\n      <td>-3.389095</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>490a81728c3228478e8f9196a09d485cd8e06b8c0512c4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I bought a possible answer to a homework probl...</td>\n      <td>5.0</td>\n      <td>-4.648178</td>\n      <td>-4.786387</td>\n      <td>-1.271109</td>\n      <td>-2.308218</td>\n      <td>-4.468682</td>\n      <td>0.195158</td>\n      <td>-2.507853</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>7db158133f1a4d68c59dc693051bfe63c47c80d906dc01...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>Not being a good host for a friend from a far ...</td>\n      <td>3.0</td>\n      <td>-3.077817</td>\n      <td>-5.027366</td>\n      <td>-3.306948</td>\n      <td>1.170537</td>\n      <td>-5.571788</td>\n      <td>-1.354663</td>\n      <td>-2.787492</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>e9534a6e936543dcc8437a79899a804ffce79dac3054ee...</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>I had a very horrible dream one night, I dream...</td>\n      <td>1.0</td>\n      <td>-4.497149</td>\n      <td>2.975088</td>\n      <td>-3.899056</td>\n      <td>-3.790592</td>\n      <td>-4.291361</td>\n      <td>-3.033495</td>\n      <td>-3.544418</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>d1a9fe0005e2d8b96fe3a97c87621da377fd26d585efd4...</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>My sweetheart left me, or rather we decided to...</td>\n      <td>0.0</td>\n      <td>2.665590</td>\n      <td>-4.259548</td>\n      <td>-3.877426</td>\n      <td>-3.458135</td>\n      <td>-3.568334</td>\n      <td>-3.607715</td>\n      <td>-3.872411</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>9715d835adb73aac1dbf805e9e8474997a614da4222656...</td>\n    </tr>\n    <tr>\n      <th>1143</th>\n      <td>My boyfriend made me a proposal.</td>\n      <td>4.0</td>\n      <td>-4.251548</td>\n      <td>-4.527634</td>\n      <td>-3.848884</td>\n      <td>-4.058742</td>\n      <td>3.007751</td>\n      <td>-3.362574</td>\n      <td>-4.002860</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>9b088be713afaffe47f1b751f79cf0e584f93d8613819e...</td>\n    </tr>\n    <tr>\n      <th>1144</th>\n      <td>I am disgusted with liers, hypocrites, slander...</td>\n      <td>6.0</td>\n      <td>-3.587851</td>\n      <td>-3.542270</td>\n      <td>-3.460089</td>\n      <td>-3.469225</td>\n      <td>-3.664184</td>\n      <td>-3.535312</td>\n      <td>3.006108</td>\n      <td>6.0</td>\n      <td>1</td>\n      <td>a39a81be5c0586a3eb01525370fefbaee556665c8a1099...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7619 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.468895\n3   -1.524600\n4   -2.693809\n5   -3.241329\n6   -3.736658\n7   -4.332664\n8   -4.967506\nName: 0, dtype: float64"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_df.groupby(1).mean().iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          2         3         4         5         6         7         8\n",
      "1                                                                      \n",
      "0  0.468895 -1.524600 -2.693809 -3.241329 -3.736658 -4.332664 -4.967506\n",
      "1  2.139970 -2.654546 -3.304612 -3.592845 -3.829158 -4.109175 -4.398682\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                      0  1         2  \\\n7     e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...  0 -0.008224   \n9     508013a422a1328897e746c1081fde9677a652d3726f03...  0 -0.728218   \n30    5101e942f4f13396228afcfc0ed7022a0bd6bae205467f...  0  2.535155   \n45    d7485e1149e3b505c3e36a20ea33e77c43703507178737...  0  0.248427   \n84    53c81747c3fca04f8f5b61fef58e98a2343ec64e8c48eb...  0  0.694850   \n...                                                 ... ..       ...   \n7603  de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...  0 -0.097690   \n7606  e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...  0  0.082269   \n7609  a83f80f12520ed9e13e989966c65741004f035375b6a11...  0 -0.001578   \n7611  626cb9b2d46c7417b520262f774105604125788a28f8bd...  0  0.357695   \n7613  cf08f5346552c6a3f23b06c68c6ac445335249040de4af...  0  2.233496   \n\n             3         4         5         6         7         8  \n7    -0.964764 -1.882689 -2.903396 -3.956276 -5.073253 -5.711443  \n9    -1.289517 -1.292279 -1.551765 -1.902784 -2.547890 -3.734523  \n30   -3.758141 -3.882107 -4.036671 -4.077008 -4.282697 -4.305556  \n45   -1.630920 -1.910735 -4.273924 -4.447170 -4.692871 -5.099661  \n84   -2.536959 -2.952217 -3.167273 -4.052315 -4.325487 -5.477589  \n...        ...       ...       ...       ...       ...       ...  \n7603 -1.457908 -2.605385 -3.104316 -3.726416 -4.161256 -5.285932  \n7606  0.077403 -3.585347 -3.779755 -4.522589 -4.537704 -4.587114  \n7609 -1.585250 -2.177770 -3.840732 -4.270634 -4.719012 -5.603081  \n7611 -0.800455 -2.666971 -3.686851 -4.200370 -4.346138 -5.021138  \n7613 -2.198705 -3.595109 -3.652084 -3.808757 -4.050680 -4.488806  \n\n[1447 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...</td>\n      <td>0</td>\n      <td>-0.008224</td>\n      <td>-0.964764</td>\n      <td>-1.882689</td>\n      <td>-2.903396</td>\n      <td>-3.956276</td>\n      <td>-5.073253</td>\n      <td>-5.711443</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>0</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>-1.292279</td>\n      <td>-1.551765</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-3.734523</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>5101e942f4f13396228afcfc0ed7022a0bd6bae205467f...</td>\n      <td>0</td>\n      <td>2.535155</td>\n      <td>-3.758141</td>\n      <td>-3.882107</td>\n      <td>-4.036671</td>\n      <td>-4.077008</td>\n      <td>-4.282697</td>\n      <td>-4.305556</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>d7485e1149e3b505c3e36a20ea33e77c43703507178737...</td>\n      <td>0</td>\n      <td>0.248427</td>\n      <td>-1.630920</td>\n      <td>-1.910735</td>\n      <td>-4.273924</td>\n      <td>-4.447170</td>\n      <td>-4.692871</td>\n      <td>-5.099661</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>53c81747c3fca04f8f5b61fef58e98a2343ec64e8c48eb...</td>\n      <td>0</td>\n      <td>0.694850</td>\n      <td>-2.536959</td>\n      <td>-2.952217</td>\n      <td>-3.167273</td>\n      <td>-4.052315</td>\n      <td>-4.325487</td>\n      <td>-5.477589</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7603</th>\n      <td>de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...</td>\n      <td>0</td>\n      <td>-0.097690</td>\n      <td>-1.457908</td>\n      <td>-2.605385</td>\n      <td>-3.104316</td>\n      <td>-3.726416</td>\n      <td>-4.161256</td>\n      <td>-5.285932</td>\n    </tr>\n    <tr>\n      <th>7606</th>\n      <td>e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...</td>\n      <td>0</td>\n      <td>0.082269</td>\n      <td>0.077403</td>\n      <td>-3.585347</td>\n      <td>-3.779755</td>\n      <td>-4.522589</td>\n      <td>-4.537704</td>\n      <td>-4.587114</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>a83f80f12520ed9e13e989966c65741004f035375b6a11...</td>\n      <td>0</td>\n      <td>-0.001578</td>\n      <td>-1.585250</td>\n      <td>-2.177770</td>\n      <td>-3.840732</td>\n      <td>-4.270634</td>\n      <td>-4.719012</td>\n      <td>-5.603081</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>626cb9b2d46c7417b520262f774105604125788a28f8bd...</td>\n      <td>0</td>\n      <td>0.357695</td>\n      <td>-0.800455</td>\n      <td>-2.666971</td>\n      <td>-3.686851</td>\n      <td>-4.200370</td>\n      <td>-4.346138</td>\n      <td>-5.021138</td>\n    </tr>\n    <tr>\n      <th>7613</th>\n      <td>cf08f5346552c6a3f23b06c68c6ac445335249040de4af...</td>\n      <td>0</td>\n      <td>2.233496</td>\n      <td>-2.198705</td>\n      <td>-3.595109</td>\n      <td>-3.652084</td>\n      <td>-3.808757</td>\n      <td>-4.050680</td>\n      <td>-4.488806</td>\n    </tr>\n  </tbody>\n</table>\n<p>1447 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(conf_df.groupby(1).mean())\n",
    "conf_df[conf_df[1] == 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               sentence  ytrue       2_x  \\\n0     I went to a pub with a group of friends (not v...    3.0 -3.956276   \n1                                       [ No response.]    0.0 -1.902784   \n2                                       [ No response.]    0.0 -1.902784   \n3                                       [ No response.]    0.0 -1.902784   \n4                                       [ No response.]    0.0 -1.902784   \n...                                                 ...    ...       ...   \n4562  Our Headmaster came to our hostel. The lights ...    3.0 -5.285932   \n4563  Every time I was around this one person I woul...    5.0 -4.587114   \n4564  When I quarrelled with a very close friend of ...    0.0 -4.270634   \n4565               Adultery with a friend's girlfriend.    3.0 -4.200370   \n4566  On the T.V I saw a news feature on South Afric...    2.0 -4.050680   \n\n           3_x       4_x       5_x       6_x       7_x       8_x  predictions  \\\n0    -5.711443 -0.008224 -2.903396 -5.073253 -1.882689 -0.964764          2.0   \n1    -2.547890 -1.551765 -1.292279 -3.734523 -0.728218 -1.289517          5.0   \n2    -2.547890 -1.551765 -1.292279 -3.734523 -0.728218 -1.289517          5.0   \n3    -2.547890 -1.551765 -1.292279 -3.734523 -0.728218 -1.289517          5.0   \n4    -2.547890 -1.551765 -1.292279 -3.734523 -0.728218 -1.289517          5.0   \n...        ...       ...       ...       ...       ...       ...          ...   \n4562 -1.457908 -2.605385 -3.104316 -4.161256 -0.097690 -3.726416          5.0   \n4563 -4.522589 -3.779755  0.082269 -4.537704  0.077403 -3.585347          3.0   \n4564 -4.719012 -1.585250 -0.001578 -5.603081 -2.177770 -3.840732          3.0   \n4565 -4.346138 -3.686851 -0.800455 -5.021138  0.357695 -2.666971          5.0   \n4566 -3.808757 -2.198705 -3.652084 -4.488806 -3.595109  2.233496          6.0   \n\n      ...                                                _id  \\\n0     ...  e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...   \n1     ...  508013a422a1328897e746c1081fde9677a652d3726f03...   \n2     ...  508013a422a1328897e746c1081fde9677a652d3726f03...   \n3     ...  508013a422a1328897e746c1081fde9677a652d3726f03...   \n4     ...  508013a422a1328897e746c1081fde9677a652d3726f03...   \n...   ...                                                ...   \n4562  ...  de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...   \n4563  ...  e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...   \n4564  ...  a83f80f12520ed9e13e989966c65741004f035375b6a11...   \n4565  ...  626cb9b2d46c7417b520262f774105604125788a28f8bd...   \n4566  ...  cf08f5346552c6a3f23b06c68c6ac445335249040de4af...   \n\n                                                      0  1       2_y  \\\n0     e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...  0 -0.008224   \n1     508013a422a1328897e746c1081fde9677a652d3726f03...  0 -0.728218   \n2     508013a422a1328897e746c1081fde9677a652d3726f03...  0 -0.728218   \n3     508013a422a1328897e746c1081fde9677a652d3726f03...  0 -0.728218   \n4     508013a422a1328897e746c1081fde9677a652d3726f03...  0 -0.728218   \n...                                                 ... ..       ...   \n4562  de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...  0 -0.097690   \n4563  e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...  0  0.082269   \n4564  a83f80f12520ed9e13e989966c65741004f035375b6a11...  0 -0.001578   \n4565  626cb9b2d46c7417b520262f774105604125788a28f8bd...  0  0.357695   \n4566  cf08f5346552c6a3f23b06c68c6ac445335249040de4af...  0  2.233496   \n\n           3_y       4_y       5_y       6_y       7_y       8_y  \n0    -0.964764 -1.882689 -2.903396 -3.956276 -5.073253 -5.711443  \n1    -1.289517 -1.292279 -1.551765 -1.902784 -2.547890 -3.734523  \n2    -1.289517 -1.292279 -1.551765 -1.902784 -2.547890 -3.734523  \n3    -1.289517 -1.292279 -1.551765 -1.902784 -2.547890 -3.734523  \n4    -1.289517 -1.292279 -1.551765 -1.902784 -2.547890 -3.734523  \n...        ...       ...       ...       ...       ...       ...  \n4562 -1.457908 -2.605385 -3.104316 -3.726416 -4.161256 -5.285932  \n4563  0.077403 -3.585347 -3.779755 -4.522589 -4.537704 -4.587114  \n4564 -1.585250 -2.177770 -3.840732 -4.270634 -4.719012 -5.603081  \n4565 -0.800455 -2.666971 -3.686851 -4.200370 -4.346138 -5.021138  \n4566 -2.198705 -3.595109 -3.652084 -3.808757 -4.050680 -4.488806  \n\n[4567 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>ytrue</th>\n      <th>2_x</th>\n      <th>3_x</th>\n      <th>4_x</th>\n      <th>5_x</th>\n      <th>6_x</th>\n      <th>7_x</th>\n      <th>8_x</th>\n      <th>predictions</th>\n      <th>...</th>\n      <th>_id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2_y</th>\n      <th>3_y</th>\n      <th>4_y</th>\n      <th>5_y</th>\n      <th>6_y</th>\n      <th>7_y</th>\n      <th>8_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I went to a pub with a group of friends (not v...</td>\n      <td>3.0</td>\n      <td>-3.956276</td>\n      <td>-5.711443</td>\n      <td>-0.008224</td>\n      <td>-2.903396</td>\n      <td>-5.073253</td>\n      <td>-1.882689</td>\n      <td>-0.964764</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...</td>\n      <td>e529d0e588faf319a8d2a56bce1cfbaa33e95bd9675f2a...</td>\n      <td>0</td>\n      <td>-0.008224</td>\n      <td>-0.964764</td>\n      <td>-1.882689</td>\n      <td>-2.903396</td>\n      <td>-3.956276</td>\n      <td>-5.073253</td>\n      <td>-5.711443</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[ No response.]</td>\n      <td>0.0</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-1.551765</td>\n      <td>-1.292279</td>\n      <td>-3.734523</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>0</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>-1.292279</td>\n      <td>-1.551765</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-3.734523</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[ No response.]</td>\n      <td>0.0</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-1.551765</td>\n      <td>-1.292279</td>\n      <td>-3.734523</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>0</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>-1.292279</td>\n      <td>-1.551765</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-3.734523</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[ No response.]</td>\n      <td>0.0</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-1.551765</td>\n      <td>-1.292279</td>\n      <td>-3.734523</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>0</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>-1.292279</td>\n      <td>-1.551765</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-3.734523</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[ No response.]</td>\n      <td>0.0</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-1.551765</td>\n      <td>-1.292279</td>\n      <td>-3.734523</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>508013a422a1328897e746c1081fde9677a652d3726f03...</td>\n      <td>0</td>\n      <td>-0.728218</td>\n      <td>-1.289517</td>\n      <td>-1.292279</td>\n      <td>-1.551765</td>\n      <td>-1.902784</td>\n      <td>-2.547890</td>\n      <td>-3.734523</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4562</th>\n      <td>Our Headmaster came to our hostel. The lights ...</td>\n      <td>3.0</td>\n      <td>-5.285932</td>\n      <td>-1.457908</td>\n      <td>-2.605385</td>\n      <td>-3.104316</td>\n      <td>-4.161256</td>\n      <td>-0.097690</td>\n      <td>-3.726416</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...</td>\n      <td>de310b231e98ef7333e9db336d15f4776bbc7657c75a7c...</td>\n      <td>0</td>\n      <td>-0.097690</td>\n      <td>-1.457908</td>\n      <td>-2.605385</td>\n      <td>-3.104316</td>\n      <td>-3.726416</td>\n      <td>-4.161256</td>\n      <td>-5.285932</td>\n    </tr>\n    <tr>\n      <th>4563</th>\n      <td>Every time I was around this one person I woul...</td>\n      <td>5.0</td>\n      <td>-4.587114</td>\n      <td>-4.522589</td>\n      <td>-3.779755</td>\n      <td>0.082269</td>\n      <td>-4.537704</td>\n      <td>0.077403</td>\n      <td>-3.585347</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...</td>\n      <td>e1fe362705f21a14b6357fbb2efa698ff2a8d12b50630c...</td>\n      <td>0</td>\n      <td>0.082269</td>\n      <td>0.077403</td>\n      <td>-3.585347</td>\n      <td>-3.779755</td>\n      <td>-4.522589</td>\n      <td>-4.537704</td>\n      <td>-4.587114</td>\n    </tr>\n    <tr>\n      <th>4564</th>\n      <td>When I quarrelled with a very close friend of ...</td>\n      <td>0.0</td>\n      <td>-4.270634</td>\n      <td>-4.719012</td>\n      <td>-1.585250</td>\n      <td>-0.001578</td>\n      <td>-5.603081</td>\n      <td>-2.177770</td>\n      <td>-3.840732</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>a83f80f12520ed9e13e989966c65741004f035375b6a11...</td>\n      <td>a83f80f12520ed9e13e989966c65741004f035375b6a11...</td>\n      <td>0</td>\n      <td>-0.001578</td>\n      <td>-1.585250</td>\n      <td>-2.177770</td>\n      <td>-3.840732</td>\n      <td>-4.270634</td>\n      <td>-4.719012</td>\n      <td>-5.603081</td>\n    </tr>\n    <tr>\n      <th>4565</th>\n      <td>Adultery with a friend's girlfriend.</td>\n      <td>3.0</td>\n      <td>-4.200370</td>\n      <td>-4.346138</td>\n      <td>-3.686851</td>\n      <td>-0.800455</td>\n      <td>-5.021138</td>\n      <td>0.357695</td>\n      <td>-2.666971</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>626cb9b2d46c7417b520262f774105604125788a28f8bd...</td>\n      <td>626cb9b2d46c7417b520262f774105604125788a28f8bd...</td>\n      <td>0</td>\n      <td>0.357695</td>\n      <td>-0.800455</td>\n      <td>-2.666971</td>\n      <td>-3.686851</td>\n      <td>-4.200370</td>\n      <td>-4.346138</td>\n      <td>-5.021138</td>\n    </tr>\n    <tr>\n      <th>4566</th>\n      <td>On the T.V I saw a news feature on South Afric...</td>\n      <td>2.0</td>\n      <td>-4.050680</td>\n      <td>-3.808757</td>\n      <td>-2.198705</td>\n      <td>-3.652084</td>\n      <td>-4.488806</td>\n      <td>-3.595109</td>\n      <td>2.233496</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>cf08f5346552c6a3f23b06c68c6ac445335249040de4af...</td>\n      <td>cf08f5346552c6a3f23b06c68c6ac445335249040de4af...</td>\n      <td>0</td>\n      <td>2.233496</td>\n      <td>-2.198705</td>\n      <td>-3.595109</td>\n      <td>-3.652084</td>\n      <td>-3.808757</td>\n      <td>-4.050680</td>\n      <td>-4.488806</td>\n    </tr>\n  </tbody>\n</table>\n<p>4567 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_match = bert_all[bert_all[\"match\"] == 0].reset_index(drop=True)\n",
    "non_match = non_match.merge(conf_df[conf_df[1] == 0], left_on=\"_id\", right_on=0)\n",
    "non_match"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ItishaYadav1/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Text(0.5, 0, 'Normal Distribution'), Text(0, 0.5, 'Frequency')]"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for inline plots in jupyter\n",
    "%matplotlib inline\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# for latex equations\n",
    "from IPython.display import Math, Latex\n",
    "# for displaying images\n",
    "from IPython.core.display import Image\n",
    "\n",
    "# import seaborn\n",
    "import seaborn as sns\n",
    "# settings for seaborn plotting style\n",
    "sns.set(color_codes=True)\n",
    "# settings for seaborn plot sizes\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "ax = sns.distplot(conf_df.groupby(1).mean().iloc[0],\n",
    "                  kde=True,\n",
    "                  color='skyblue'\n",
    "                  )\n",
    "ax.set(xlabel='Normal Distribution', ylabel='Frequency')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ninp_data = test\\nbert_out = bert_test\\n\\nassert inp_data.shape[0] == bert_out.shape[0]\\n\\nanalysis_df = inp_data.merge(bert_out, left_index=True, right_index=True)\\nanalysis_df = analysis_df[analysis_df[\"anger\"]]\\n\\n'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Applying analysis on test\n",
    "\"\"\"\n",
    "\n",
    "# inp_data = pd.concat([test, val])\n",
    "# bert_out = pd.concat([bert_test, bert_val])\n",
    "\n",
    "\n",
    "inp_data = test\n",
    "bert_out = bert_test\n",
    "\n",
    "assert inp_data.shape[0] == bert_out.shape[0]\n",
    "\n",
    "analysis_df = inp_data.merge(bert_out, left_index=True, right_index=True)\n",
    "analysis_df = analysis_df[analysis_df[\"anger\"]]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0    -4.143373 -4.099311 -4.180825 -3.873905  3.270709 -3.883025 -3.976190   \n1    -4.454488  2.634034 -3.970481 -3.600777 -4.260800 -3.508614 -4.351669   \n2    -3.871844 -4.237987 -4.067472 -3.916490  3.443163 -4.022819 -3.743179   \n3    -4.045258  3.200204 -3.871938 -3.878467 -4.119332 -3.877209 -3.389095   \n4    -4.648178 -4.786387 -1.271109 -2.308218 -4.468682  0.195158 -2.507853   \n...        ...       ...       ...       ...       ...       ...       ...   \n7614 -3.077817 -5.027366 -3.306948  1.170537 -5.571788 -1.354663 -2.787492   \n7615 -4.497149  2.975088 -3.899056 -3.790592 -4.291361 -3.033495 -3.544418   \n7616  2.665590 -4.259548 -3.877426 -3.458135 -3.568334 -3.607715 -3.872411   \n7617 -4.251548 -4.527634 -3.848884 -4.058742  3.007751 -3.362574 -4.002860   \n7618 -3.587851 -3.542270 -3.460089 -3.469225 -3.664184 -3.535312  3.006108   \n\n       7    8    9    10   11   12   13  \n0     0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n1     0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n2     0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n3     0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n4     0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n...   ...  ...  ...  ...  ...  ...  ...  \n7614  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n7615  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n7616  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n7617  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n7618  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n\n[7619 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.143373</td>\n      <td>-4.099311</td>\n      <td>-4.180825</td>\n      <td>-3.873905</td>\n      <td>3.270709</td>\n      <td>-3.883025</td>\n      <td>-3.976190</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-4.454488</td>\n      <td>2.634034</td>\n      <td>-3.970481</td>\n      <td>-3.600777</td>\n      <td>-4.260800</td>\n      <td>-3.508614</td>\n      <td>-4.351669</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-3.871844</td>\n      <td>-4.237987</td>\n      <td>-4.067472</td>\n      <td>-3.916490</td>\n      <td>3.443163</td>\n      <td>-4.022819</td>\n      <td>-3.743179</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-4.045258</td>\n      <td>3.200204</td>\n      <td>-3.871938</td>\n      <td>-3.878467</td>\n      <td>-4.119332</td>\n      <td>-3.877209</td>\n      <td>-3.389095</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4.648178</td>\n      <td>-4.786387</td>\n      <td>-1.271109</td>\n      <td>-2.308218</td>\n      <td>-4.468682</td>\n      <td>0.195158</td>\n      <td>-2.507853</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7614</th>\n      <td>-3.077817</td>\n      <td>-5.027366</td>\n      <td>-3.306948</td>\n      <td>1.170537</td>\n      <td>-5.571788</td>\n      <td>-1.354663</td>\n      <td>-2.787492</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7615</th>\n      <td>-4.497149</td>\n      <td>2.975088</td>\n      <td>-3.899056</td>\n      <td>-3.790592</td>\n      <td>-4.291361</td>\n      <td>-3.033495</td>\n      <td>-3.544418</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7616</th>\n      <td>2.665590</td>\n      <td>-4.259548</td>\n      <td>-3.877426</td>\n      <td>-3.458135</td>\n      <td>-3.568334</td>\n      <td>-3.607715</td>\n      <td>-3.872411</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7617</th>\n      <td>-4.251548</td>\n      <td>-4.527634</td>\n      <td>-3.848884</td>\n      <td>-4.058742</td>\n      <td>3.007751</td>\n      <td>-3.362574</td>\n      <td>-4.002860</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7618</th>\n      <td>-3.587851</td>\n      <td>-3.542270</td>\n      <td>-3.460089</td>\n      <td>-3.469225</td>\n      <td>-3.664184</td>\n      <td>-3.535312</td>\n      <td>3.006108</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7619 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6172\n",
      "0    6172\n",
      "Name: 1, dtype: int64\n",
      "[0.6970196  0.14109169 0.08099354]\n",
      "(12344, 3)\n",
      "12344\n",
      "(9875, 3) (9875,) (2469, 3) (2469,)\n",
      "Train Accuracy: 78.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80      6172\n",
      "           1       0.82      0.74      0.78      6172\n",
      "\n",
      "    accuracy                           0.79     12344\n",
      "   macro avg       0.79      0.79      0.79     12344\n",
      "weighted avg       0.79      0.79      0.79     12344\n",
      "\n",
      "[[5139 1033]\n",
      " [1589 4583]]\n",
      "\n",
      "\n",
      "Test Accuracy: 78.78%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80      1235\n",
      "           1       0.82      0.74      0.78      1234\n",
      "\n",
      "    accuracy                           0.79      2469\n",
      "   macro avg       0.79      0.79      0.79      2469\n",
      "weighted avg       0.79      0.79      0.79      2469\n",
      "\n",
      "[[1028  207]\n",
      " [ 317  917]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "           dim1      dim2      dim3  yhat  y_pred_prob_1  y_pred_prob_2  \\\n11932  1.594322 -0.023795 -1.156041     0       0.888671       0.111329   \n1896   0.069710 -0.345975  0.621636     0       0.528909       0.471091   \n11676 -1.076294 -0.500565 -0.644399     1       0.246130       0.753870   \n5695  -1.581426 -0.054975 -0.518705     1       0.077281       0.922719   \n12094  0.078587 -1.407673  1.721454     0       0.608567       0.391433   \n...         ...       ...       ...   ...            ...            ...   \n5305  -0.283464  0.122823  0.643270     1       0.312340       0.687660   \n11294  1.387510  3.876121  2.509586     0       0.890139       0.109861   \n9294   1.217284  0.078961 -1.337352     0       0.854516       0.145484   \n12330  0.338228 -1.035749  0.097836     0       0.682393       0.317607   \n5566  -1.660102 -0.082655 -0.501314     1       0.068173       0.931827   \n\n       ytrue  match  \n11932      0      1  \n1896       0      1  \n11676      0      0  \n5695       1      1  \n12094      0      1  \n...      ...    ...  \n5305       1      1  \n11294      0      1  \n9294       0      1  \n12330      0      1  \n5566       1      1  \n\n[2469 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dim1</th>\n      <th>dim2</th>\n      <th>dim3</th>\n      <th>yhat</th>\n      <th>y_pred_prob_1</th>\n      <th>y_pred_prob_2</th>\n      <th>ytrue</th>\n      <th>match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11932</th>\n      <td>1.594322</td>\n      <td>-0.023795</td>\n      <td>-1.156041</td>\n      <td>0</td>\n      <td>0.888671</td>\n      <td>0.111329</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1896</th>\n      <td>0.069710</td>\n      <td>-0.345975</td>\n      <td>0.621636</td>\n      <td>0</td>\n      <td>0.528909</td>\n      <td>0.471091</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11676</th>\n      <td>-1.076294</td>\n      <td>-0.500565</td>\n      <td>-0.644399</td>\n      <td>1</td>\n      <td>0.246130</td>\n      <td>0.753870</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5695</th>\n      <td>-1.581426</td>\n      <td>-0.054975</td>\n      <td>-0.518705</td>\n      <td>1</td>\n      <td>0.077281</td>\n      <td>0.922719</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12094</th>\n      <td>0.078587</td>\n      <td>-1.407673</td>\n      <td>1.721454</td>\n      <td>0</td>\n      <td>0.608567</td>\n      <td>0.391433</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5305</th>\n      <td>-0.283464</td>\n      <td>0.122823</td>\n      <td>0.643270</td>\n      <td>1</td>\n      <td>0.312340</td>\n      <td>0.687660</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11294</th>\n      <td>1.387510</td>\n      <td>3.876121</td>\n      <td>2.509586</td>\n      <td>0</td>\n      <td>0.890139</td>\n      <td>0.109861</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9294</th>\n      <td>1.217284</td>\n      <td>0.078961</td>\n      <td>-1.337352</td>\n      <td>0</td>\n      <td>0.854516</td>\n      <td>0.145484</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12330</th>\n      <td>0.338228</td>\n      <td>-1.035749</td>\n      <td>0.097836</td>\n      <td>0</td>\n      <td>0.682393</td>\n      <td>0.317607</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5566</th>\n      <td>-1.660102</td>\n      <td>-0.082655</td>\n      <td>-0.501314</td>\n      <td>1</td>\n      <td>0.068173</td>\n      <td>0.931827</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2469 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def mlp_classifier(X, y):\n",
    "\n",
    "    \"\"\"Perform PCA\"\"\"\n",
    "\n",
    "\n",
    "    oversample = SMOTE()\n",
    "\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    print(y.value_counts())\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_data = pca.fit_transform(X)\n",
    "    reduced_data_df = pd.DataFrame(reduced_data, columns=[\"dim1\", \"dim2\", \"dim3\"])\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    data_col = reduced_data_df.columns\n",
    "    scalar = StandardScaler()\n",
    "\n",
    "    scalar.fit(reduced_data_df)\n",
    "    # transform data\n",
    "    standard_data = scalar.transform(reduced_data_df)\n",
    "    standard_df = pd.DataFrame(standard_data, columns=data_col)\n",
    "    print(standard_df.shape)\n",
    "    print(len(y))\n",
    "    \"\"\"\n",
    "    Data Splitting\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(standard_df, y, test_size = 0.2,\n",
    "                                                        random_state = 42, stratify = y)\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    \"\"\"Model training\"\"\"\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(max_iter=100, random_state=42, activation=\"tanh\", solver=\"adam\", hidden_layer_sizes=(100, ),\n",
    "    learning_rate=\"adaptive\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    \"\"\"Predictions\"\"\"\n",
    "\n",
    "    y_pred_train = clf.predict(standard_df)\n",
    "    y_pred_train_proba = clf.predict_proba(standard_df)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y, y_pred_train)\n",
    "    print(\"Train Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(classification_report(y, y_pred_train))\n",
    "\n",
    "    print(confusion_matrix(y, y_pred_train))\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    top_df_test = X_test\n",
    "    top_df_test[\"yhat\"] = y_pred\n",
    "    top_df_test[[\"y_pred_prob_1\", \"y_pred_prob_2\"]] = y_pred_prob\n",
    "    top_df_test[\"ytrue\"] = y_test\n",
    "    top_match = []\n",
    "    for ind, row in top_df_test.iterrows():\n",
    "        if row[\"ytrue\"] == row[\"yhat\"]:\n",
    "            top_match.append(1)\n",
    "        else:\n",
    "            top_match.append(0)\n",
    "    top_df_test[\"match\"] = top_match\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return top_df_test, pca, scalar, clf\n",
    "\n",
    "\"\"\"\n",
    "define y\n",
    "\"\"\"\n",
    "# transform the dataset\n",
    "import pickle\n",
    "a, pca, scalar, clf = mlp_classifier(conf_df.drop(columns=[0, 1]).reset_index(drop=True), conf_df[1])\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Strategy to approach the research question:\n",
    "    1. Filter the dataset (all) to find misclassified examples.\n",
    "    2. Find out the those specific sentences and see manually, if they are the potential examples of class overlapping.\n",
    "    3. Train the ORD detector, and see of the correct label for missclassfied classes are in the top 3 recommendation or not?\n",
    "    4. Evaluate the bert model, using our custom evaluator\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "As discussed with Prof, go with approach one and put approach 2 in future work\n",
    "ORD:\n",
    "\n",
    "1. BERT\n",
    "2. Low_vs_high confidence detector\n",
    "\n",
    "Soft decision maker:\n",
    "\n",
    "1. Approach 1: Rule Based\n",
    "\n",
    "    If overlapping region detected:\n",
    "\n",
    "    Recommend top 2 emotions\n",
    "\n",
    "    If non overlapping region detected:\n",
    "\n",
    "    Recommend top emotion\n",
    "\n",
    "2. Approach 2: Machine learning based\n",
    "    1. Train Classifiers:\n",
    "        1. Classifier 1 checks if top 2 can be recommended.\n",
    "        2. Classifier 2 checks, if top 3 can be recommended.\n",
    "        3. Worst case senario: model not confident with the emotion.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "match  ytrue\n0      0         207\n       1         317\n1      0        1028\n       1         917\nName: dim1, dtype: int64"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.groupby([\"match\", \"ytrue\"]).count()[\"dim1\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}