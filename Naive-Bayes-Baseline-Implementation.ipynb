{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nUsing Language Models\\n1. Assumption for Naive Bayes : All words are independent\\nslide no 38.\\n2. Naive Bayes is a probabilistic classifier.\\n3. We compute the probability of document being in class C.\\n4. The goal of the naive bayes class is to find the best class.\\n5. Naive Bayes = probability of each class + prob of each token belonging to that class.\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using Language Models\n",
    "1. Assumption for Naive Bayes : All words are independent\n",
    "slide no 38.\n",
    "2. Naive Bayes is a probabilistic classifier.\n",
    "3. We compute the probability of document being in class C.\n",
    "4. The goal of the naive bayes class is to find the best class.\n",
    "5. Naive Bayes = probability of each class + prob of each token belonging to that class.\n",
    "6. Formula = argmax(log(P(c) + log(P(t/c)))\n",
    "7. Smoothing in NB\n",
    "8. P(t/c) = T(no of particular token) + 1 / T(sum of all tokens in a class) + V(size of vocab)\n",
    "9. Naive bayes is good for predicting the class and not for estimating probabilites.\n",
    "10. Naive Bayes is robust to concept drift. (change of definition of class over time).\n",
    "11. For text, the independence assumption does not hold for naive bayes, but for other domains it does hold.\n",
    "12. Advantages:\n",
    "    1. Very Fast\n",
    "    2. Low storage requirement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ItishaYadav1/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 119: expected 2 fields, saw 7\\nSkipping line 1213: expected 2 fields, saw 4\\nSkipping line 2323: expected 2 fields, saw 3\\nSkipping line 2803: expected 2 fields, saw 3\\nSkipping line 3630: expected 2 fields, saw 4\\nSkipping line 4635: expected 2 fields, saw 5\\nSkipping line 4797: expected 2 fields, saw 4\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": "4.0    777\n3.0    766\n7.0    760\n2.0    758\n5.0    757\n6.0    757\n1.0    751\nName: Y, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mapping = {\"fear\": 1, \"anger\": 2, \"guilt\": 3, \"joy\": 4, \"shame\": 5, \"disgust\": 6, \"sadness\": 7}\n",
    "isear_train_df = pd.read_csv(\"./isear/isear-train.csv\", error_bad_lines=False, header=None)\n",
    "\n",
    "mapped_emotions = isear_train_df[0].map(mapping)\n",
    "isear_train_df[0] = mapped_emotions\n",
    "isear_train_df = isear_train_df.dropna().reset_index(drop=True)\n",
    "isear_train_df = isear_train_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "#isear_train_df = isear_train_df[isear_train_df[\"Y\"].isin([4, 7])]\n",
    "isear_train_df[\"Y\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocabSize = 0\n",
    "        self.trainingSize = 0\n",
    "        self.vocab = []\n",
    "        self.fear = {}\n",
    "        self.anger = {}\n",
    "        self.guilt = {}\n",
    "        self.joy = {}\n",
    "        self.shame = {}\n",
    "        self.disgust = {}\n",
    "        self.sadness = {}\n",
    "        self.fearSize = 0\n",
    "        self.angerSize = 0\n",
    "        self.guiltSize = 0\n",
    "        self.joySize = 0\n",
    "        self.shameSize = 0\n",
    "        self.disgustSize = 0\n",
    "        self.sadnessSize = 0\n",
    "        self.priorDict = {}\n",
    "        self.stopwords = {'hers', 'below', \"wouldn't\", 'nor', 'for', 'over', \"hasn't\", 'at', 'shouldn', 'only', 'above', 'itself', 'yourselves', 'what', \"don't\", \"it's\", 'which', 'against', \"that'll\", 'has', 'i', 'his', 'having', 'then', \"shan't\", 'myself', 'do', 'yours', 'up', 'own', 'the', 'same', 'aren', 'few', 'through', 'here', 'whom', 'o', \"aren't\", 'were', 'are', 'both', \"didn't\", 'll', 'again', 'is', 're', \"wasn't\", \"you'll\", 'm', \"haven't\", 'such', 'off', 'of', 'it', 'did', 'into', 'to', 'other', 'was', 'just', 've', \"mustn't\", 'while', 'about', 'each', 'by', 'this', 'isn', 'ourselves', 'in', 'our', 'couldn', 'until', 'where', \"couldn't\", 'ain', \"you'd\", 'all', 'when', 'does', 'before', 'weren', 'y', 'doing', 'than', 'being', 'my', 'mightn', 'yourself', 'with', 'theirs', 'so', \"needn't\", 'a', \"doesn't\", \"isn't\", 'its', 'your', 'if', \"should've\", 'ma', 'can', 'herself', 'but', 'too', 'more', 'her', \"hadn't\", 'hadn', 'there', \"you're\", 'from', 'should', 'we', 'how', 'out', 'once', 'mustn', 'won', 'their', 'don', 'had', 'he', 'or', 'didn', 'd', 'down', 't', \"she's\", 'that', 'himself', 'wouldn', \"you've\", \"mightn't\", 'between', 'them', 'on', 'haven', 'after', 'themselves', 'because', 'and', 'you', 'very', 's', 'these', 'no', 'now', 'him', 'been', 'those', 'during', 'doesn', 'wasn', 'am', 'under', 'an', 'some', 'have', 'me', 'any', 'who', 'shan', 'why', 'will', \"shouldn't\", 'not', 'they', \"won't\", 'needn', 'further', 'most', 'be', 'ours', 'she', 'as', 'hasn', \"weren't\", \"a\", \"''\"}\n",
    "\n",
    "        self.classMapping = {1: \"fear\", 2: \"anger\", 3: \"guilt\", 4: \"joy\", 5: \"shame\", 6: \"disgust\", 7: \"sadness\"}\n",
    "\n",
    "    def getVocabulary(self, text):\n",
    "        tokens = [re.sub(\"[^A-Za-z]\", \"\",i).strip().lower() for i in text.split(\" \") if i not in self.stopwords and len(i) > 1]\n",
    "        return list(set(tokens))\n",
    "\n",
    "\n",
    "    def updateProbabilityDict(self, emodf, resDict):\n",
    "        text = \" \".join(list(emodf[\"X\"])).split(\" \")\n",
    "        for word in text:\n",
    "            word = re.sub(\"[^A-Za-z]\", \"\", word).strip().lower()\n",
    "            if word not in self.stopwords and len(word) > 1:\n",
    "                if word in resDict:\n",
    "                    resDict[word] += 1\n",
    "                else:\n",
    "                    resDict[word] = 1\n",
    "        size = sum([value for key, value in resDict.items()])\n",
    "        return resDict, size\n",
    "\n",
    "\n",
    "    def maximum_likelihood_estimation(self, instance, emoDict, emotionCorpusSize, emotion):\n",
    "        \"\"\"\n",
    "        instanceDict = {}\n",
    "        for word in instance.split(\" \"):\n",
    "            word = re.sub(\"[^A-Za-z]\", \"\", word).strip().lower()\n",
    "            if word not in self.stopwords and len(word) > 1:\n",
    "                if word in instanceDict:\n",
    "                    instanceDict[word] += 1\n",
    "                else:\n",
    "                    instanceDict[word] = 1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        p_tc = 0\n",
    "        tokens = [re.sub(\"[^A-Za-z]\", \"\",i).strip().lower() for i in instance.split(\" \") if i not in self.stopwords and len(i) > 1]\n",
    "        for word in tokens:\n",
    "            if word in emoDict:\n",
    "                p_tc += (emoDict[word] + 1) / (emotionCorpusSize + self.vocabSize)\n",
    "            else:\n",
    "                p_tc += 1 / (emotionCorpusSize + self.vocabSize)\n",
    "        return np.log(self.priorDict[emotion]) + np.log(p_tc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"\n",
    "        tokenDict... : holds the probability of occurence of each word in the class, therefore the length of dict = vocab size.\n",
    "        priorDict: holds the probability of occurence of each class\n",
    "        \"\"\"\n",
    "        self.vocab = self.getVocabulary(\" \".join(list(train_df[\"X\"])))\n",
    "        self.vocabSize = len(self.vocab)\n",
    "        self.trainingSize = train_df.shape[0]\n",
    "        self.priorDict = {\"fear\": train_df[train_df[\"Y\"] == 1].shape[0]/self.trainingSize,\n",
    "                          \"anger\": train_df[train_df[\"Y\"] == 2].shape[0]/self.trainingSize,\n",
    "                           \"guilt\": train_df[train_df[\"Y\"] == 3].shape[0]/self.trainingSize,\n",
    "                           \"joy\": train_df[train_df[\"Y\"] == 4].shape[0]/self.trainingSize,\n",
    "                           \"shame\": train_df[train_df[\"Y\"] == 5].shape[0]/self.trainingSize,\n",
    "                           \"disgust\": train_df[train_df[\"Y\"] == 6].shape[0]/self.trainingSize,\n",
    "                           \"sadness\": train_df[train_df[\"Y\"] == 7].shape[0]/self.trainingSize,\n",
    "                          }\n",
    "        self.fear, self.fearSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 1], self.fear)\n",
    "        self.anger, self.angerSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 2], self.anger)\n",
    "        self.guilt, self.guiltSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 3], self.guilt)\n",
    "        self.joy, self.joySize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 4], self.joy)\n",
    "        self.shame, self.shameSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 5], self.shame)\n",
    "        self.disgust, self.disgustSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 6], self.disgust)\n",
    "        self.sadness, self.sadnessSize = self.updateProbabilityDict(train_df[train_df[\"Y\"] == 7], self.sadness)\n",
    "\n",
    "\n",
    "    def predict(self, xtest):\n",
    "        \"\"\"\n",
    "        This method uses the probabilites learned in the fit function and applies the formula to get the correct class for the given instance.\n",
    "        \"\"\"\n",
    "\n",
    "        k = 0\n",
    "        predictions = []\n",
    "        for instance in xtest:\n",
    "            k += 1\n",
    "            #print(k, instance)\n",
    "            prob = []\n",
    "\n",
    "            proba_fear = self.maximum_likelihood_estimation(instance, self.fear, emotionCorpusSize=self.fearSize, emotion=\"fear\")\n",
    "            prob.append(proba_fear)\n",
    "            proba_anger = self.maximum_likelihood_estimation(instance, self.anger, emotionCorpusSize=self.angerSize, emotion=\"anger\")\n",
    "            prob.append(proba_anger)\n",
    "            proba_guilt = self.maximum_likelihood_estimation(instance, self.guilt, emotionCorpusSize=self.guiltSize, emotion=\"guilt\")\n",
    "            prob.append(proba_guilt)\n",
    "\n",
    "            proba_joy = self.maximum_likelihood_estimation(instance, self.joy, emotionCorpusSize=self.joySize, emotion=\"joy\")\n",
    "            prob.append(proba_joy)\n",
    "\n",
    "            proba_shame = self.maximum_likelihood_estimation(instance, self.shame, emotionCorpusSize=self.shameSize, emotion=\"shame\")\n",
    "            prob.append(proba_shame)\n",
    "            proba_disgust = self.maximum_likelihood_estimation(instance, self.disgust, emotionCorpusSize=self.disgustSize, emotion=\"disgust\")\n",
    "            prob.append(proba_disgust)\n",
    "\n",
    "            proba_sadness = self.maximum_likelihood_estimation(instance, self.sadness, emotionCorpusSize=self.sadnessSize, emotion=\"sadness\")\n",
    "            prob.append(proba_sadness)\n",
    "            prediction = np.argmax(np.array(prob)) + 1\n",
    "            predictions.append(prediction)\n",
    "            #print(self.classMapping[prediction])\n",
    "        return predictions\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ItishaYadav1/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(2293, 2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "import pandas as pd\n",
    "mapping = {\"fear\": 1, \"anger\": 2, \"guilt\": 3, \"joy\": 4, \"shame\": 5, \"disgust\": 6, \"sadness\": 7}\n",
    "isear_test1_df = pd.read_csv(\"./isear/isear-test.csv\", error_bad_lines=False, header=None)\n",
    "isear_val_df = pd.read_csv(\"./isear/isear-val.csv\", error_bad_lines=False, header=None)\n",
    "isear_test_df = pd.concat([isear_test1_df, isear_val_df])\n",
    "mapped_emotions = isear_test_df[0].map(mapping)\n",
    "isear_test_df[0] = mapped_emotions\n",
    "isear_test_df = isear_test_df.dropna().reset_index(drop=True)\n",
    "isear_test_df = isear_test_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "#isear_test_df = isear_test_df[isear_test_df[\"Y\"].isin([4, 7])]\n",
    "isear_test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ntemp = []\\nfor i in predictions:\\n    if i == 1:\\n        temp.append(4)\\n    elif i == 2:\\n        temp.append(7)\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(isear_train_df[\"X\"], isear_train_df[\"Y\"], stratify=isear_train_df[\"Y\"], test_size=0.2, random_state=42)\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"X\"] = X_train\n",
    "train_df[\"Y\"] = y_train\n",
    "print(train_df.shape)\n",
    "print(y_test.shape)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\"\"\"\n",
    "\n",
    "nbObj = NaiveBayesClassifier()\n",
    "nbObj.fit(isear_train_df)\n",
    "predictions = nbObj.predict(isear_test_df[\"X\"])\n",
    "\"\"\"\n",
    "temp = []\n",
    "for i in predictions:\n",
    "    if i == 1:\n",
    "        temp.append(4)\n",
    "    elif i == 2:\n",
    "        temp.append(7)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[2,\n 3,\n 3,\n 6,\n 4,\n 3,\n 2,\n 7,\n 3,\n 7,\n 1,\n 5,\n 7,\n 7,\n 4,\n 7,\n 3,\n 2,\n 3,\n 2,\n 7,\n 1,\n 6,\n 1,\n 4,\n 7,\n 6,\n 3,\n 4,\n 2,\n 4,\n 5,\n 7,\n 7,\n 1,\n 1,\n 4,\n 3,\n 7,\n 1,\n 5,\n 4,\n 5,\n 6,\n 3,\n 2,\n 3,\n 4,\n 2,\n 5,\n 3,\n 2,\n 2,\n 5,\n 5,\n 7,\n 4,\n 2,\n 4,\n 7,\n 4,\n 1,\n 5,\n 4,\n 3,\n 7,\n 6,\n 5,\n 7,\n 4,\n 3,\n 4,\n 3,\n 3,\n 2,\n 3,\n 7,\n 7,\n 7,\n 1,\n 1,\n 4,\n 2,\n 4,\n 1,\n 2,\n 3,\n 7,\n 5,\n 5,\n 2,\n 5,\n 3,\n 3,\n 4,\n 2,\n 1,\n 3,\n 5,\n 7,\n 6,\n 7,\n 2,\n 2,\n 2,\n 7,\n 4,\n 3,\n 1,\n 6,\n 1,\n 2,\n 5,\n 4,\n 1,\n 3,\n 6,\n 3,\n 3,\n 3,\n 7,\n 4,\n 3,\n 5,\n 3,\n 4,\n 3,\n 1,\n 2,\n 1,\n 7,\n 7,\n 7,\n 4,\n 2,\n 1,\n 4,\n 3,\n 2,\n 4,\n 4,\n 2,\n 2,\n 5,\n 1,\n 6,\n 6,\n 4,\n 2,\n 2,\n 1,\n 4,\n 4,\n 3,\n 7,\n 6,\n 4,\n 5,\n 7,\n 5,\n 1,\n 2,\n 6,\n 5,\n 4,\n 6,\n 1,\n 3,\n 7,\n 5,\n 7,\n 4,\n 4,\n 2,\n 4,\n 3,\n 7,\n 6,\n 2,\n 4,\n 4,\n 7,\n 7,\n 5,\n 7,\n 6,\n 4,\n 7,\n 3,\n 7,\n 4,\n 1,\n 3,\n 4,\n 3,\n 4,\n 2,\n 6,\n 7,\n 7,\n 1,\n 3,\n 3,\n 5,\n 3,\n 2,\n 1,\n 6,\n 4,\n 1,\n 3,\n 6,\n 1,\n 4,\n 1,\n 3,\n 4,\n 6,\n 7,\n 5,\n 6,\n 5,\n 4,\n 6,\n 3,\n 5,\n 7,\n 6,\n 2,\n 4,\n 1,\n 7,\n 7,\n 4,\n 4,\n 7,\n 6,\n 2,\n 4,\n 7,\n 4,\n 7,\n 2,\n 3,\n 2,\n 1,\n 4,\n 2,\n 5,\n 1,\n 3,\n 7,\n 4,\n 1,\n 3,\n 3,\n 6,\n 2,\n 6,\n 4,\n 5,\n 3,\n 6,\n 3,\n 7,\n 7,\n 7,\n 3,\n 2,\n 4,\n 3,\n 3,\n 3,\n 4,\n 3,\n 4,\n 5,\n 6,\n 5,\n 6,\n 7,\n 4,\n 1,\n 3,\n 4,\n 3,\n 4,\n 7,\n 5,\n 5,\n 1,\n 4,\n 1,\n 5,\n 2,\n 6,\n 2,\n 6,\n 7,\n 2,\n 1,\n 7,\n 2,\n 1,\n 6,\n 6,\n 1,\n 5,\n 1,\n 2,\n 7,\n 2,\n 4,\n 5,\n 1,\n 7,\n 2,\n 5,\n 4,\n 4,\n 7,\n 7,\n 3,\n 6,\n 5,\n 7,\n 3,\n 7,\n 7,\n 4,\n 2,\n 2,\n 4,\n 7,\n 1,\n 7,\n 6,\n 6,\n 3,\n 4,\n 3,\n 5,\n 3,\n 2,\n 3,\n 6,\n 7,\n 1,\n 2,\n 5,\n 4,\n 4,\n 4,\n 3,\n 3,\n 7,\n 6,\n 4,\n 4,\n 6,\n 1,\n 2,\n 1,\n 3,\n 7,\n 3,\n 5,\n 1,\n 1,\n 5,\n 1,\n 1,\n 1,\n 6,\n 4,\n 4,\n 4,\n 7,\n 4,\n 6,\n 4,\n 7,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 7,\n 6,\n 3,\n 5,\n 1,\n 7,\n 4,\n 3,\n 2,\n 7,\n 3,\n 4,\n 1,\n 7,\n 1,\n 1,\n 3,\n 5,\n 6,\n 3,\n 6,\n 3,\n 3,\n 6,\n 3,\n 6,\n 3,\n 1,\n 4,\n 2,\n 1,\n 1,\n 7,\n 6,\n 7,\n 2,\n 7,\n 3,\n 5,\n 1,\n 4,\n 1,\n 2,\n 1,\n 1,\n 3,\n 6,\n 5,\n 3,\n 4,\n 3,\n 1,\n 4,\n 7,\n 2,\n 6,\n 1,\n 7,\n 1,\n 1,\n 5,\n 1,\n 6,\n 2,\n 5,\n 3,\n 3,\n 7,\n 7,\n 7,\n 6,\n 7,\n 7,\n 7,\n 4,\n 3,\n 6,\n 2,\n 1,\n 6,\n 2,\n 1,\n 6,\n 6,\n 2,\n 3,\n 2,\n 4,\n 4,\n 1,\n 3,\n 6,\n 5,\n 4,\n 7,\n 6,\n 3,\n 7,\n 6,\n 7,\n 1,\n 7,\n 3,\n 5,\n 5,\n 2,\n 3,\n 6,\n 2,\n 4,\n 3,\n 7,\n 3,\n 6,\n 4,\n 7,\n 3,\n 3,\n 4,\n 4,\n 6,\n 4,\n 4,\n 7,\n 4,\n 3,\n 5,\n 3,\n 7,\n 7,\n 7,\n 1,\n 4,\n 6,\n 6,\n 7,\n 5,\n 3,\n 5,\n 4,\n 3,\n 3,\n 7,\n 1,\n 7,\n 7,\n 7,\n 1,\n 3,\n 4,\n 1,\n 4,\n 1,\n 5,\n 4,\n 1,\n 4,\n 2,\n 4,\n 4,\n 3,\n 4,\n 4,\n 1,\n 4,\n 5,\n 7,\n 1,\n 2,\n 1,\n 6,\n 1,\n 4,\n 5,\n 1,\n 4,\n 3,\n 7,\n 7,\n 1,\n 1,\n 3,\n 4,\n 4,\n 4,\n 4,\n 1,\n 6,\n 1,\n 1,\n 3,\n 7,\n 1,\n 7,\n 3,\n 5,\n 1,\n 3,\n 1,\n 2,\n 3,\n 4,\n 5,\n 7,\n 7,\n 3,\n 2,\n 6,\n 6,\n 7,\n 1,\n 2,\n 6,\n 3,\n 2,\n 6,\n 4,\n 1,\n 1,\n 3,\n 7,\n 3,\n 6,\n 3,\n 7,\n 3,\n 4,\n 5,\n 1,\n 2,\n 3,\n 1,\n 1,\n 4,\n 2,\n 7,\n 3,\n 3,\n 7,\n 4,\n 1,\n 5,\n 1,\n 1,\n 7,\n 3,\n 2,\n 7,\n 6,\n 6,\n 7,\n 7,\n 5,\n 1,\n 5,\n 7,\n 4,\n 7,\n 4,\n 4,\n 2,\n 7,\n 6,\n 5,\n 1,\n 6,\n 2,\n 7,\n 1,\n 2,\n 3,\n 7,\n 5,\n 5,\n 5,\n 3,\n 7,\n 2,\n 5,\n 1,\n 5,\n 6,\n 4,\n 6,\n 4,\n 7,\n 3,\n 6,\n 7,\n 4,\n 2,\n 1,\n 4,\n 6,\n 4,\n 7,\n 3,\n 1,\n 4,\n 4,\n 4,\n 1,\n 3,\n 3,\n 4,\n 4,\n 7,\n 4,\n 4,\n 5,\n 2,\n 7,\n 1,\n 7,\n 2,\n 2,\n 3,\n 3,\n 6,\n 3,\n 6,\n 7,\n 4,\n 3,\n 2,\n 4,\n 2,\n 5,\n 7,\n 4,\n 5,\n 3,\n 7,\n 3,\n 6,\n 4,\n 4,\n 6,\n 4,\n 2,\n 5,\n 6,\n 5,\n 6,\n 3,\n 1,\n 4,\n 2,\n 4,\n 7,\n 6,\n 5,\n 3,\n 5,\n 4,\n 1,\n 3,\n 2,\n 6,\n 1,\n 4,\n 4,\n 5,\n 1,\n 3,\n 2,\n 3,\n 5,\n 5,\n 6,\n 1,\n 6,\n 6,\n 3,\n 6,\n 2,\n 6,\n 4,\n 1,\n 3,\n 4,\n 7,\n 2,\n 5,\n 1,\n 1,\n 4,\n 4,\n 3,\n 4,\n 4,\n 3,\n 3,\n 6,\n 7,\n 4,\n 4,\n 7,\n 2,\n 3,\n 3,\n 4,\n 7,\n 5,\n 2,\n 4,\n 3,\n 3,\n 4,\n 4,\n 2,\n 4,\n 2,\n 5,\n 4,\n 7,\n 3,\n 3,\n 1,\n 1,\n 6,\n 5,\n 1,\n 4,\n 3,\n 4,\n 5,\n 4,\n 7,\n 3,\n 1,\n 5,\n 1,\n 5,\n 3,\n 6,\n 5,\n 3,\n 3,\n 3,\n 4,\n 1,\n 5,\n 3,\n 2,\n 4,\n 5,\n 1,\n 3,\n 3,\n 3,\n 3,\n 6,\n 7,\n 2,\n 3,\n 7,\n 7,\n 7,\n 3,\n 5,\n 7,\n 4,\n 1,\n 4,\n 3,\n 5,\n 3,\n 6,\n 4,\n 2,\n 3,\n 2,\n 6,\n 4,\n 1,\n 1,\n 3,\n 1,\n 1,\n 4,\n 1,\n 3,\n 3,\n 4,\n 6,\n 3,\n 2,\n 7,\n 7,\n 4,\n 7,\n 4,\n 2,\n 1,\n 1,\n 4,\n 2,\n 7,\n 2,\n 3,\n 4,\n 7,\n 7,\n 1,\n 6,\n 4,\n 1,\n 3,\n 5,\n 1,\n 6,\n 2,\n 6,\n 3,\n 4,\n 2,\n 7,\n 4,\n 3,\n 2,\n 3,\n 3,\n 6,\n 2,\n 7,\n 3,\n 4,\n 7,\n 4,\n 3,\n 3,\n 6,\n 5,\n 4,\n 2,\n 1,\n 1,\n 4,\n 2,\n 5,\n 6,\n 1,\n 6,\n 3,\n 1,\n 3,\n 1,\n 3,\n 7,\n 2,\n 1,\n 4,\n 3,\n 3,\n 1,\n 2,\n 2,\n 3,\n 6,\n 2,\n 6,\n 1,\n 3,\n 2,\n 4,\n 7,\n 1,\n 7,\n 1,\n 7,\n 7,\n 6,\n 3,\n 4,\n 2,\n 4,\n 1,\n 4,\n 5,\n 6,\n 4,\n 2,\n 3,\n 7,\n 6,\n 6,\n 3,\n 1,\n 2,\n 7,\n 4,\n 2,\n 3,\n 1,\n 2,\n 7,\n 5,\n 6,\n 3,\n 4,\n 4,\n 6,\n 4,\n 5,\n 7,\n 1,\n 5,\n 7,\n 2,\n 4,\n 3,\n ...]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Recall\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class evaluation:\n",
    "    def __init__(self, y_actual, y_pred):\n",
    "        self.y_actual = np.array(y_actual)\n",
    "        #print(self.y_actual)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        #print(self.y_pred)\n",
    "\n",
    "    def confusion_matrix(self, actual, pred):\n",
    "        tp = fp = tn = fn = 0\n",
    "        for i, j in zip(actual, pred):\n",
    "            if i == 1:\n",
    "                # positive\n",
    "                if i == j:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                # negative\n",
    "                if i == j:\n",
    "                    tn += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "        cf = pd.DataFrame([[tp, fp], [fn, tn]], columns=[\"actual_pos\", \"actual_neg\"], index=[\"pred_pos\", \"pred_neg\"])\n",
    "        return cf, tp, fp, tn, fn\n",
    "\n",
    "    def recall(self, actual, pred):\n",
    "        cf, tp, fp, tn, fn = self.confusion_matrix(actual, pred)\n",
    "        return round(tp / (tp+fn), 2)\n",
    "\n",
    "    def precision(self, actual, pred):\n",
    "        cf, tp, fp, tn, fn = self.confusion_matrix(actual, pred)\n",
    "        return round(tp / (tp+fp), 2)\n",
    "\n",
    "    def f1(self, actual, pred):\n",
    "        # harmonic mean\n",
    "        pr = self.precision(actual, pred)\n",
    "        re = self.recall(actual, pred)\n",
    "        f1 = 2 * ((pr * re) / (pr + re))\n",
    "        return round(f1, 2)\n",
    "\n",
    "    def main(self):\n",
    "        mapping = {1: \"fear\", 2: \"anger\", 3: \"guilt\", 4: \"joy\", 5: \"shame\", 6: \"disgust\", 7: \"sadness\"}\n",
    "        res = {}\n",
    "        for cls in [1.0, 2.0, 3.0, 4.0, 5.0, 6.0 ,7.0]:\n",
    "            c = 0\n",
    "            #print(\"Class : \", mapping[cls])\n",
    "            mod_y_actual = []\n",
    "            for i in self.y_actual:\n",
    "                if i == cls:\n",
    "                    c += 1\n",
    "                    mod_y_actual.append(1)\n",
    "                else:\n",
    "                    mod_y_actual.append(0)\n",
    "            mod_y_pred = []\n",
    "            for i in self.y_pred:\n",
    "                if i == cls:\n",
    "                    mod_y_pred.append(1)\n",
    "                else:\n",
    "                    mod_y_pred.append(0)\n",
    "            #print(mod_y_pred)\n",
    "            \"\"\"\n",
    "            print()\n",
    "            print()\n",
    "            print(\"Confusion Matrix : \\n\", self.confusion_matrix(mod_y_actual, mod_y_pred)[0])\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"Precision : \\n\", self.precision(mod_y_actual, mod_y_pred))\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"Recall : \\n\", self.recall(mod_y_actual, mod_y_pred))\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"F1 Score : \\n\", self.f1(mod_y_actual, mod_y_pred))\n",
    "            print()\n",
    "            print()\n",
    "        \"\"\"\n",
    "            temp = [self.precision(mod_y_actual, mod_y_pred), self.recall(mod_y_actual, mod_y_pred), self.f1(mod_y_actual, mod_y_pred), c]\n",
    "            res[mapping[cls]] = temp\n",
    "        res = pd.DataFrame(res, index=[\"Precision\", \"Recall\", \"F1-Score\", \"Count\"]).transpose()\n",
    "\n",
    "        \"\"\"\n",
    "        Macro average calculation\n",
    "        \"\"\"\n",
    "        avg_pr = np.sum((res[\"Precision\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        avg_re = np.sum((res[\"Recall\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        avg_f1 = np.sum((res[\"F1-Score\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        res.loc[\"Macro_Average\"] = [round(avg_pr, 2), round(avg_re, 2), round(avg_f1, 2), np.sum(res[\"Count\"])]\n",
    "        #res = pd.concat([res, pd.DataFrame([avg_pr, avg_re, avg_f1, np.sum(res[\"Count\"])]).transpose()])\n",
    "\n",
    "        flattened_index = [\"Fear-Precision\", \"Fear-Recall\", \"Fear-F1score\", \"Fear-Count\",\n",
    "                           \"Anger-Precision\", \"Anger-Recall\", \"Anger-F1score\", \"Anger-Count\",\n",
    "                           \"Guilt-Precision\", \"Guilt-Recall\", \"Guilt-F1score\", \"Guilt-Count\",\n",
    "                           \"Joy-Precision\", \"Joy-Recall\", \"Joy-F1score\", \"Joy-Count\",\n",
    "                           \"Shame-Precision\", \"Shame-Recall\", \"Shame-F1score\", \"Shame-Count\",\n",
    "                           \"Disgust-Precision\", \"Disgust-Recall\", \"Disgust-F1score\", \"Disgust-Count\",\n",
    "                           \"Sadness-Precision\", \"Sadness-Recall\", \"Sadness-F1score\", \"Sadness-Count\",\n",
    "                           \"Macro-Average-Precision\", \"Macro-Average-Recall\", \"Macro-Average-F1score\", \"Macro-Average-Count\"]\n",
    "        res_flattened = pd.DataFrame(res.to_numpy().flatten(), index=flattened_index).transpose()\n",
    "        return res_flattened, res\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "               Precision  Recall  F1-Score   Count\nfear                0.56    0.55      0.55   335.0\nanger               0.33    0.41      0.37   336.0\nguilt               0.45    0.34      0.39   320.0\njoy                 0.64    0.48      0.55   313.0\nshame               0.34    0.53      0.41   331.0\ndisgust             0.44    0.54      0.48   332.0\nsadness             0.57    0.52      0.54   326.0\nMacro_Average       0.47    0.48      0.47  2293.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fear</th>\n      <td>0.56</td>\n      <td>0.55</td>\n      <td>0.55</td>\n      <td>335.0</td>\n    </tr>\n    <tr>\n      <th>anger</th>\n      <td>0.33</td>\n      <td>0.41</td>\n      <td>0.37</td>\n      <td>336.0</td>\n    </tr>\n    <tr>\n      <th>guilt</th>\n      <td>0.45</td>\n      <td>0.34</td>\n      <td>0.39</td>\n      <td>320.0</td>\n    </tr>\n    <tr>\n      <th>joy</th>\n      <td>0.64</td>\n      <td>0.48</td>\n      <td>0.55</td>\n      <td>313.0</td>\n    </tr>\n    <tr>\n      <th>shame</th>\n      <td>0.34</td>\n      <td>0.53</td>\n      <td>0.41</td>\n      <td>331.0</td>\n    </tr>\n    <tr>\n      <th>disgust</th>\n      <td>0.44</td>\n      <td>0.54</td>\n      <td>0.48</td>\n      <td>332.0</td>\n    </tr>\n    <tr>\n      <th>sadness</th>\n      <td>0.57</td>\n      <td>0.52</td>\n      <td>0.54</td>\n      <td>326.0</td>\n    </tr>\n    <tr>\n      <th>Macro_Average</th>\n      <td>0.47</td>\n      <td>0.48</td>\n      <td>0.47</td>\n      <td>2293.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evalObj = evaluation(y_actual=isear_test_df[\"Y\"], y_pred=predictions)\n",
    "evalObj.main()[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "               Precision    Recall  F1-Score   Count\nfear            0.640000  0.578313  0.607595   150.0\nanger           0.355263  0.461538  0.401487   152.0\nguilt           0.450980  0.367021  0.404692   153.0\njoy             0.628205  0.507772  0.561605   156.0\nshame           0.342105  0.530612  0.416000   152.0\ndisgust         0.476821  0.590164  0.527473   151.0\nsadness         0.644737  0.538462  0.586826   152.0\nMacro_Average   0.505629  0.510208  0.500724  1066.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fear</th>\n      <td>0.640000</td>\n      <td>0.578313</td>\n      <td>0.607595</td>\n      <td>150.0</td>\n    </tr>\n    <tr>\n      <th>anger</th>\n      <td>0.355263</td>\n      <td>0.461538</td>\n      <td>0.401487</td>\n      <td>152.0</td>\n    </tr>\n    <tr>\n      <th>guilt</th>\n      <td>0.450980</td>\n      <td>0.367021</td>\n      <td>0.404692</td>\n      <td>153.0</td>\n    </tr>\n    <tr>\n      <th>joy</th>\n      <td>0.628205</td>\n      <td>0.507772</td>\n      <td>0.561605</td>\n      <td>156.0</td>\n    </tr>\n    <tr>\n      <th>shame</th>\n      <td>0.342105</td>\n      <td>0.530612</td>\n      <td>0.416000</td>\n      <td>152.0</td>\n    </tr>\n    <tr>\n      <th>disgust</th>\n      <td>0.476821</td>\n      <td>0.590164</td>\n      <td>0.527473</td>\n      <td>151.0</td>\n    </tr>\n    <tr>\n      <th>sadness</th>\n      <td>0.644737</td>\n      <td>0.538462</td>\n      <td>0.586826</td>\n      <td>152.0</td>\n    </tr>\n    <tr>\n      <th>Macro_Average</th>\n      <td>0.505629</td>\n      <td>0.510208</td>\n      <td>0.500724</td>\n      <td>1066.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c = 0\n",
    "for i, j in zip(y_test, predictions):\n",
    "    if int(i) == j:\n",
    "        c += 1\n",
    "print(c/len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  18  27  45   6  22  28]\n",
      " [ 41 111  72  24  25  34  29]\n",
      " [ 27  46 144  26  29  17  31]\n",
      " [ 17  13  36 199   6  10  32]\n",
      " [ 19  29  70  40 114  33  26]\n",
      " [ 23  33  41  35  25 147  28]\n",
      " [ 25  21  30  42  11  11 186]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(isear_test_df[\"Y\"], predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "c = 0\n",
    "for i, j in zip(isear_test_df[\"Y\"], temp):\n",
    "    if int(i) == j:\n",
    "        c += 1\n",
    "print(c/len(isear_test_df[\"Y\"]))\n",
    "\n",
    "\"accuracy with only joy and sadness = 82%, with 80:20 train:test split\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5056285178236398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = 0\n",
    "for i, j in zip(isear_test_df[\"Y\"], temp):\n",
    "    if int(i) == j:\n",
    "        c += 1\n",
    "print(c/len(isear_test_df[\"Y\"]))\n",
    "\n",
    "\"accuracy with only joy and sadness = 82%, with 80:20 train:test split\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = 0\n",
    "for i, j in zip(y_test, temp):\n",
    "    if int(i) == j:\n",
    "        c += 1\n",
    "print(c/len(y_test))\n",
    "\n",
    "\"accuracy with only joy and sadness = 82%, with 80:20 train:test split\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}