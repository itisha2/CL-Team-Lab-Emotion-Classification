{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nUsing Language Models\\n1. Assumption for Naive Bayes : All words are independent\\nslide no 38.\\n2. Naive Bayes is a probabilistic classifier.\\n3. We compute the probability of document being in class C.\\n4. The goal of the naive bayes class is to find the best class.\\n5. Naive Bayes = probability of each class + prob of each token belonging to that class.\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using Language Models\n",
    "1. Assumption for Naive Bayes : All words are independent\n",
    "slide no 38.\n",
    "2. Naive Bayes is a probabilistic classifier.\n",
    "3. We compute the probability of document being in class C.\n",
    "4. The goal of the naive bayes class is to find the best class.\n",
    "5. Naive Bayes = probability of each class + prob of each token belonging to that class.\n",
    "6. Formula = argmax(log(P(c) + log(P(t/c)))\n",
    "7. Smoothing in NB\n",
    "8. P(t/c) = T(no of particular token) + 1 / T(sum of all tokens in a class) + V(size of vocab)\n",
    "9. Naive bayes is good for predicting the class and not for estimating probabilites.\n",
    "10. Naive Bayes is robust to concept drift. (change of definition of class over time).\n",
    "11. For text, the independence assumption does not hold for naive bayes, but for other domains it does hold.\n",
    "12. Advantages:\n",
    "    1. Very Fast\n",
    "    2. Low storage requirement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "class evaluation:\n",
    "    def __init__(self, y_actual, y_pred, labels):\n",
    "        self.y_actual = np.array(y_actual)\n",
    "        #print(self.y_actual)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        self.labels = labels\n",
    "        #print(self.y_pred)\n",
    "\n",
    "    def confusion_matrix(self, actual, pred):\n",
    "        tp = fp = tn = fn = 0\n",
    "        for i, j in zip(actual, pred):\n",
    "            if i == 1:\n",
    "                # positive\n",
    "                if i == j:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                # negative\n",
    "                if i == j:\n",
    "                    tn += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "        cf = pd.DataFrame([[tp, fp], [fn, tn]], columns=[\"actual_pos\", \"actual_neg\"], index=[\"pred_pos\", \"pred_neg\"])\n",
    "        return cf, tp, fp, tn, fn\n",
    "\n",
    "    def recall(self, actual, pred):\n",
    "        cf, tp, fp, tn, fn = self.confusion_matrix(actual, pred)\n",
    "        return round(tp / (tp+fn), 2)\n",
    "\n",
    "    def precision(self, actual, pred):\n",
    "        cf, tp, fp, tn, fn = self.confusion_matrix(actual, pred)\n",
    "        return round(tp / (tp+fp), 2)\n",
    "\n",
    "    def f1(self, actual, pred):\n",
    "        # harmonic mean\n",
    "        pr = self.precision(actual, pred)\n",
    "        re = self.recall(actual, pred)\n",
    "        f1 = 2 * ((pr * re) / (pr + re))\n",
    "        return round(f1, 2)\n",
    "\n",
    "    def main(self):\n",
    "        #mapping = {1: \"fear\", 2: \"anger\", 3: \"guilt\", 4: \"joy\", 5: \"shame\", 6: \"disgust\", 7: \"sadness\"}\n",
    "        res = {}\n",
    "        for cls in self.labels:\n",
    "            c = 0\n",
    "            #print(\"Class : \", mapping[cls])\n",
    "            mod_y_actual = []\n",
    "            for i in self.y_actual:\n",
    "                if i == cls:\n",
    "                    c += 1\n",
    "                    mod_y_actual.append(1)\n",
    "                else:\n",
    "                    mod_y_actual.append(0)\n",
    "\n",
    "            mod_y_pred = []\n",
    "            for i in self.y_pred:\n",
    "                if i == cls:\n",
    "                    mod_y_pred.append(1)\n",
    "                else:\n",
    "                    mod_y_pred.append(0)\n",
    "            #print(mod_y_pred)\n",
    "            \"\"\"\n",
    "            print()\n",
    "            print()\n",
    "            print(\"Confusion Matrix : \\n\", self.confusion_matrix(mod_y_actual, mod_y_pred)[0])\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"Precision : \\n\", self.precision(mod_y_actual, mod_y_pred))\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"Recall : \\n\", self.recall(mod_y_actual, mod_y_pred))\n",
    "            print(\"*******************************************************\\n\")\n",
    "            print(\"F1 Score : \\n\", self.f1(mod_y_actual, mod_y_pred))\n",
    "            print()\n",
    "            print()\n",
    "        \"\"\"\n",
    "            temp = [self.precision(mod_y_actual, mod_y_pred), self.recall(mod_y_actual, mod_y_pred), self.f1(mod_y_actual, mod_y_pred), c]\n",
    "            res[cls] = temp\n",
    "        res = pd.DataFrame(res, index=[\"Precision\", \"Recall\", \"F1-Score\", \"Count\"]).transpose()\n",
    "\n",
    "        \"\"\"\n",
    "        Macro average calculation\n",
    "        \"\"\"\n",
    "        avg_pr = np.sum((res[\"Precision\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        avg_re = np.sum((res[\"Recall\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        avg_f1 = np.sum((res[\"F1-Score\"]*res[\"Count\"]))/ np.sum(res[\"Count\"])\n",
    "        res.loc[\"macro_average\"] = [round(avg_pr, 2), round(avg_re, 2), round(avg_f1, 2), np.sum(res[\"Count\"])]\n",
    "        #res = pd.concat([res, pd.DataFrame([avg_pr, avg_re, avg_f1, np.sum(res[\"Count\"])]).transpose()])\n",
    "        flattened_index = []\n",
    "        for fl_lab in self.labels:\n",
    "            flattened_index.append(fl_lab+\"-precision\")\n",
    "            flattened_index.append(fl_lab+\"-recall\")\n",
    "            flattened_index.append(fl_lab+\"-f1score\")\n",
    "            flattened_index.append(fl_lab+\"-count\")\n",
    "        flattened_index = flattened_index + [\"macro-average-precision\", \"macro-average-recall\", \"macro-average-F1score\", \"macro-average-count\"]\n",
    "        res_flattened = pd.DataFrame(res.to_numpy().flatten(), index=flattened_index).transpose()\n",
    "        return res_flattened, res\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocabSize = 0\n",
    "        self.trainingSize = 0\n",
    "        self.train_labels = set()\n",
    "        self.vocab = []\n",
    "        self.fear = {}\n",
    "        self.anger = {}\n",
    "        self.guilt = {}\n",
    "        self.joy = {}\n",
    "        self.shame = {}\n",
    "        self.disgust = {}\n",
    "        self.sadness = {}\n",
    "        self.fearSize = 0\n",
    "        self.angerSize = 0\n",
    "        self.guiltSize = 0\n",
    "        self.joySize = 0\n",
    "        self.shameSize = 0\n",
    "        self.disgustSize = 0\n",
    "        self.sadnessSize = 0\n",
    "        self.priorDict = {}\n",
    "        self.stopwords = {'hers', 'below', \"wouldn't\", 'nor', 'for', 'over', \"hasn't\", 'at', 'shouldn', 'only', 'above', 'itself', 'yourselves', 'what', \"don't\", \"it's\", 'which', 'against', \"that'll\", 'has', 'i', 'his', 'having', 'then', \"shan't\", 'myself', 'do', 'yours', 'up', 'own', 'the', 'same', 'aren', 'few', 'through', 'here', 'whom', 'o', \"aren't\", 'were', 'are', 'both', \"didn't\", 'll', 'again', 'is', 're', \"wasn't\", \"you'll\", 'm', \"haven't\", 'such', 'off', 'of', 'it', 'did', 'into', 'to', 'other', 'was', 'just', 've', \"mustn't\", 'while', 'about', 'each', 'by', 'this', 'isn', 'ourselves', 'in', 'our', 'couldn', 'until', 'where', \"couldn't\", 'ain', \"you'd\", 'all', 'when', 'does', 'before', 'weren', 'y', 'doing', 'than', 'being', 'my', 'mightn', 'yourself', 'with', 'theirs', 'so', \"needn't\", 'a', \"doesn't\", \"isn't\", 'its', 'your', 'if', \"should've\", 'ma', 'can', 'herself', 'but', 'too', 'more', 'her', \"hadn't\", 'hadn', 'there', \"you're\", 'from', 'should', 'we', 'how', 'out', 'once', 'mustn', 'won', 'their', 'don', 'had', 'he', 'or', 'didn', 'd', 'down', 't', \"she's\", 'that', 'himself', 'wouldn', \"you've\", \"mightn't\", 'between', 'them', 'on', 'haven', 'after', 'themselves', 'because', 'and', 'you', 'very', 's', 'these', 'no', 'now', 'him', 'been', 'those', 'during', 'doesn', 'wasn', 'am', 'under', 'an', 'some', 'have', 'me', 'any', 'who', 'shan', 'why', 'will', \"shouldn't\", 'not', 'they', \"won't\", 'needn', 'further', 'most', 'be', 'ours', 'she', 'as', 'hasn', \"weren't\", \"a\", \"''\"}\n",
    "        self.paraMapping = {\"fear\": [self.fear, self.fearSize],\n",
    "                            \"anger\": [self.anger, self.angerSize],\n",
    "                            \"guilt\": [self.guilt, self.guiltSize],\n",
    "                            \"joy\": [self.joy, self.joySize],\n",
    "                            \"shame\": [self.shame, self.shameSize],\n",
    "                            \"disgust\": [self.disgust, self.disgustSize],\n",
    "                            \"sadness\": [self.sadness, self.sadnessSize]\n",
    "                            }\n",
    "        self.mapping = {\"fear\": 1, \"anger\": 2, \"guilt\": 3, \"joy\": 4, \"shame\": 5, \"disgust\": 6, \"sadness\": 7}\n",
    "        self.reverseMapping = {1: \"fear\", 2: \"anger\", 3: \"guilt\", 4: \"joy\", 5: \"shame\", 6: \"disgust\", 7: \"sadness\"}\n",
    "\n",
    "    def getVocabulary(self, text):\n",
    "        tokens = [re.sub(\"[^A-Za-z]\", \"\",i).strip().lower() for i in text.split(\" \") if i not in self.stopwords and len(i) > 1]\n",
    "        return list(set(tokens))\n",
    "\n",
    "\n",
    "    def updateProbabilityDict(self, emodf, resDict):\n",
    "        text = \" \".join(list(emodf[\"X\"])).split(\" \")\n",
    "        for word in text:\n",
    "            word = re.sub(\"[^A-Za-z]\", \"\", word).strip().lower()\n",
    "            if word not in self.stopwords and len(word) > 1:\n",
    "                if word in resDict:\n",
    "                    resDict[word] += 1\n",
    "                else:\n",
    "                    resDict[word] = 1\n",
    "        size = sum([value for key, value in resDict.items()])\n",
    "        return resDict, size\n",
    "\n",
    "\n",
    "    def maximum_likelihood_estimation(self, instance, emoDict, emotionCorpusSize, emotion):\n",
    "        \"\"\"\n",
    "        instanceDict = {}\n",
    "        for word in instance.split(\" \"):\n",
    "            word = re.sub(\"[^A-Za-z]\", \"\", word).strip().lower()\n",
    "            if word not in self.stopwords and len(word) > 1:\n",
    "                if word in instanceDict:\n",
    "                    instanceDict[word] += 1\n",
    "                else:\n",
    "                    instanceDict[word] = 1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        p_tc = 0\n",
    "        tokens = [re.sub(\"[^A-Za-z]\", \"\",i).strip().lower() for i in instance.split(\" \") if i not in self.stopwords and len(i) > 1]\n",
    "        for word in tokens:\n",
    "            if word in emoDict:\n",
    "                p_tc += (emoDict[word] + 1) / (emotionCorpusSize + self.vocabSize)\n",
    "            else:\n",
    "                p_tc += 1 / (emotionCorpusSize + self.vocabSize)\n",
    "        return np.log(self.priorDict[emotion]) + np.log(p_tc)\n",
    "\n",
    "\n",
    "    def labelEncoding(self, temp):\n",
    "        #print(temp[\"Y\"].value_counts())\n",
    "        mapped_emotions = temp[\"Y\"].map(self.mapping)\n",
    "        temp[\"Y\"] = mapped_emotions\n",
    "        temp = temp.dropna().reset_index(drop=True)\n",
    "        return temp\n",
    "\n",
    "    def cleanData(self, input_df):\n",
    "        temp = input_df[input_df[\"Y\"].isin([\"fear\", \"anger\", \"guilt\", \"joy\", \"shame\", \"disgust\", \"sadness\"])]\n",
    "        return temp\n",
    "\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"\n",
    "        tokenDict... : holds the probability of occurence of each word in the class, therefore the length of dict = vocab size.\n",
    "        priorDict: holds the probability of occurence of each class\n",
    "        \"\"\"\n",
    "\n",
    "        train_df  = self.cleanData(train_df)\n",
    "        #train_df = self.labelEncoding(train_df)\n",
    "        self.train_labels = set(train_df[\"Y\"])\n",
    "        print(\"Labels in the training data = \", self.train_labels)\n",
    "        self.vocab = self.getVocabulary(\" \".join(list(train_df[\"X\"])))\n",
    "        self.vocabSize = len(self.vocab)\n",
    "        self.trainingSize = train_df.shape[0]\n",
    "\n",
    "        for label in self.train_labels:\n",
    "            self.priorDict[label] = train_df[train_df[\"Y\"] == label].shape[0]/self.trainingSize\n",
    "            emoParam = self.paraMapping[label]\n",
    "            emoParam[0], emoParam[1] = self.updateProbabilityDict(train_df[train_df[\"Y\"] == label], emoParam[0])\n",
    "\n",
    "\n",
    "    def predict(self, xtest):\n",
    "        \"\"\"\n",
    "        This method uses the probabilites learned in the fit function and applies the formula to get the correct class for the given instance.\n",
    "        \"\"\"\n",
    "\n",
    "        k = 0\n",
    "        predictions = []\n",
    "        for instance in xtest:\n",
    "            k += 1\n",
    "            #print(k, instance)\n",
    "            prob = []\n",
    "            for label in self.train_labels:\n",
    "                emoPrm = self.paraMapping[label]\n",
    "                proba_emo = self.maximum_likelihood_estimation(instance, emoPrm[0], emotionCorpusSize=emoPrm[1], emotion=label)\n",
    "                prob.append((label, proba_emo))\n",
    "           # print(sorted(prob, key = lambda x: x[1]))\n",
    "            prediction = sorted(prob, key = lambda x: x[1], reverse=True)[0][0]\n",
    "            predictions.append(prediction)\n",
    "            #predictions.append(self.mapping[prediction])\n",
    "        return predictions\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data =  (5333, 2)\n",
      "Shape of test data =  (2293, 2)\n",
      "Labels in the training data =  {'fear', 'guilt', 'anger', 'disgust', 'shame', 'sadness', 'joy'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "               Precision  Recall  F1-Score   Count\nfear                0.56    0.55      0.55   335.0\nanger               0.33    0.41      0.37   336.0\nguilt               0.45    0.34      0.39   320.0\njoy                 0.64    0.48      0.55   313.0\nshame               0.34    0.53      0.41   331.0\ndisgust             0.44    0.54      0.48   332.0\nsadness             0.57    0.52      0.54   326.0\nmacro_average       0.47    0.48      0.47  2293.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fear</th>\n      <td>0.56</td>\n      <td>0.55</td>\n      <td>0.55</td>\n      <td>335.0</td>\n    </tr>\n    <tr>\n      <th>anger</th>\n      <td>0.33</td>\n      <td>0.41</td>\n      <td>0.37</td>\n      <td>336.0</td>\n    </tr>\n    <tr>\n      <th>guilt</th>\n      <td>0.45</td>\n      <td>0.34</td>\n      <td>0.39</td>\n      <td>320.0</td>\n    </tr>\n    <tr>\n      <th>joy</th>\n      <td>0.64</td>\n      <td>0.48</td>\n      <td>0.55</td>\n      <td>313.0</td>\n    </tr>\n    <tr>\n      <th>shame</th>\n      <td>0.34</td>\n      <td>0.53</td>\n      <td>0.41</td>\n      <td>331.0</td>\n    </tr>\n    <tr>\n      <th>disgust</th>\n      <td>0.44</td>\n      <td>0.54</td>\n      <td>0.48</td>\n      <td>332.0</td>\n    </tr>\n    <tr>\n      <th>sadness</th>\n      <td>0.57</td>\n      <td>0.52</td>\n      <td>0.54</td>\n      <td>326.0</td>\n    </tr>\n    <tr>\n      <th>macro_average</th>\n      <td>0.47</td>\n      <td>0.48</td>\n      <td>0.47</td>\n      <td>2293.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Model 1 training and evaluation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# train data loading\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "isear_train_df = pd.read_csv(\"./isear/isear-train.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_train_df = isear_train_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "#isear_train_df = isear_train_df[isear_train_df[\"Y\"].isin([4, 7])]\n",
    "print(\"Shape of training data = \", isear_train_df.shape)\n",
    "\n",
    "\n",
    "#test data loading\n",
    "\n",
    "import pandas as pd\n",
    "isear_test1_df = pd.read_csv(\"./isear/isear-test.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_val_df = pd.read_csv(\"./isear/isear-val.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_test_df = pd.concat([isear_test1_df, isear_val_df])\n",
    "isear_test_df = isear_test_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "#isear_test_df = NaiveBayesClassifier().labelEncoding(isear_test_df)\n",
    "isear_test_df = NaiveBayesClassifier().cleanData(isear_test_df)\n",
    "#isear_test_df = isear_test_df[isear_test_df[\"Y\"].isin([4, 7])]\n",
    "print(\"Shape of test data = \", isear_test_df.shape)\n",
    "\n",
    "\n",
    "trainlabels=[\"fear\", \"anger\", \"guilt\", \"joy\", \"shame\", \"disgust\", \"sadness\"]\n",
    "\n",
    "nbObj = NaiveBayesClassifier()\n",
    "nbObj.fit(isear_train_df)\n",
    "predictions = nbObj.predict(isear_test_df[\"X\"])\n",
    "evalObj = evaluation(y_actual=isear_test_df[\"Y\"], y_pred=predictions, labels=trainlabels)\n",
    "evalObj.main()[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data =  (2280, 2)\n",
      "Shape of test data =  (983, 2)\n",
      "Labels in the training data =  {'guilt', 'disgust', 'shame'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "               Precision  Recall  F1-Score  Count\ndisgust             0.55    0.71      0.62  332.0\nguilt               0.74    0.52      0.61  320.0\nshame               0.48    0.60      0.53  331.0\nmacro_average       0.59    0.61      0.59  983.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>disgust</th>\n      <td>0.55</td>\n      <td>0.71</td>\n      <td>0.62</td>\n      <td>332.0</td>\n    </tr>\n    <tr>\n      <th>guilt</th>\n      <td>0.74</td>\n      <td>0.52</td>\n      <td>0.61</td>\n      <td>320.0</td>\n    </tr>\n    <tr>\n      <th>shame</th>\n      <td>0.48</td>\n      <td>0.60</td>\n      <td>0.53</td>\n      <td>331.0</td>\n    </tr>\n    <tr>\n      <th>macro_average</th>\n      <td>0.59</td>\n      <td>0.61</td>\n      <td>0.59</td>\n      <td>983.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Model 2 training and evaluation\n",
    "\"\"\"\n",
    "\n",
    "trainlabels=[\"disgust\", \"guilt\", \"shame\"]\n",
    "\n",
    "# train data loading\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "isear_train_df = pd.read_csv(\"./isear/isear-train.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_train_df = isear_train_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "isear_train_df = isear_train_df[isear_train_df[\"Y\"].isin(trainlabels)]\n",
    "print(\"Shape of training data = \", isear_train_df.shape)\n",
    "\n",
    "\n",
    "#test data loading\n",
    "\n",
    "import pandas as pd\n",
    "isear_test1_df = pd.read_csv(\"./isear/isear-test.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_val_df = pd.read_csv(\"./isear/isear-val.csv\", error_bad_lines=False, warn_bad_lines=False, header=None)\n",
    "isear_test_df = pd.concat([isear_test1_df, isear_val_df])\n",
    "isear_test_df = isear_test_df.rename(columns={0: \"Y\", 1: \"X\"})\n",
    "isear_test_df = NaiveBayesClassifier().cleanData(isear_test_df)\n",
    "isear_test_df = isear_test_df[isear_test_df[\"Y\"].isin(trainlabels)]\n",
    "print(\"Shape of test data = \", isear_test_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nbObj = NaiveBayesClassifier()\n",
    "nbObj.fit(isear_train_df)\n",
    "predictions = nbObj.predict(isear_test_df[\"X\"])\n",
    "evalObj = evaluation(y_actual=isear_test_df[\"Y\"], y_pred=predictions, labels=trainlabels)\n",
    "evalObj.main()[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[250  85]\n",
      " [ 31 282]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(isear_test_df[\"Y\"], predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}